{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling: LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " This notebook includes the process of training an LDA (Latent Dirichlet allocation) model.\n",
    " \n",
    "#### Input:\n",
    "\n",
    "ws2_1_article_clean.csv: \n",
    "\n",
    "This dataset contains all the clean articles, obtained from the `ws2_1_data_preparation` notebook.\n",
    "\n",
    "#### Output:\n",
    "\n",
    "covid_topic_<strong>XX</strong>.html:\n",
    "\n",
    "The code will produce the LDA plot as html file where <strong>XX</strong> is the optimal number of topics obtained from analyzing the LDA results.\n",
    "\n",
    "ws_2_article_topic_<strong>XX</strong>.csv:\n",
    "\n",
    "The code will produce the LDA results as features and save them into this file where <strong>XX</strong> is the optimal number of topics. It is structured in 9 columns: an article ID, an article (original text), a number of words (in the article), a cleaned version of the text, the number of words (in the cleaned text), a publication date, a dominant topic in the article, the weight of the topic, a set of keywords related to the topic, and a topic label.\n",
    " \n",
    "\n",
    "#### Topic modeling process includes:\n",
    "\n",
    "- Include all the dependencies\n",
    "- Import clean data\n",
    "- Train the LDA model and compute the coherence metric\n",
    "- Visualize the topics\n",
    "- LDA as feature\n",
    "- Map manual labels to topics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Include all the dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "!pip install gensim\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models import CoherenceModel\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "!pip install pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "pd.options.display.max_colwidth = 200\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The path to the output folder where all the outputs will be saved\n",
    "output_path = \"/project_data/data_asset\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the clean articles\n",
    "articles = pd.read_csv(f\"{output_path}/ws2_1_article_clean.csv\")\n",
    "articles.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train LDA Model\n",
    "\n",
    "Here we train the LDA model and compute the coherence metric for a range of topic numbers. This metric calculates topic coherence for topic models which is the degree of semantic similarity between high scoring words in a topic. \n",
    "\n",
    "First, we create the term dictionary of our corpus, where every unique term is assigned an index. Then, we filter the least and most frequent words and convert the list of documents (corpus) into Document Term Matrix using the dictionary. We train LDA and obtain the number of topics where the topic coherence is the highest. Finally, we train the LDA model with the optimal number of topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [text.split() for text in articles['article_clean']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the term dictionary of courpus\n",
    "dictionary = corpora.Dictionary(words)\n",
    "\n",
    "# filter the least and most frequent words: filters if less than no_below, more than no_above\n",
    "dictionary.filter_extremes(no_below=10, no_above=0.9) \n",
    "dictionary.compactify()\n",
    "\n",
    "# convert list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
    "doc_term_matrix = [dictionary.doc2bow(word) for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train LDA, computing the coherence score for a range of topics\n",
    "coherence_scores = []\n",
    "\n",
    "for num_topics in range(2, 14, 2):\n",
    "    \n",
    "    print(f\"Number of topics: \", num_topics)\n",
    "    \n",
    "    # create the object for LDA model using gensim library\n",
    "    Lda = gensim.models.ldamulticore.LdaMulticore\n",
    "\n",
    "    # run and train LDA model on the document term matrix.\n",
    "    ldamodel = Lda(doc_term_matrix, \n",
    "                   num_topics=num_topics, \n",
    "                   id2word = dictionary, \n",
    "                   passes=20, \n",
    "                   chunksize = 2000, \n",
    "                   random_state=42,\n",
    "                   workers=6)\n",
    "    \n",
    "    # compute the coherence score\n",
    "    coherence_model = CoherenceModel(model=ldamodel, \n",
    "                                     texts=words, \n",
    "                                     dictionary=dictionary, \n",
    "                                     coherence='c_v')\n",
    "\n",
    "    coherence_lda = coherence_model.get_coherence()\n",
    "    \n",
    "    coherence_scores.append((num_topics, coherence_lda))\n",
    "\n",
    "coherence_scores = [*zip(*coherence_scores)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the coherence score for topics\n",
    "plt.plot(coherence_scores[0], coherence_scores[1], marker='o')\n",
    "plt.title('Coherence Score for Topics')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the number of topics where coherence score is the highest\n",
    "num_topics = 6\n",
    "\n",
    "# run and train LDA model on the document term matrix.\n",
    "Lda = gensim.models.ldamulticore.LdaMulticore\n",
    "\n",
    "ldamodel = Lda(doc_term_matrix, \n",
    "               num_topics=num_topics, \n",
    "               id2word=dictionary, \n",
    "               passes=20, \n",
    "               chunksize=10000, \n",
    "               random_state=42,\n",
    "               workers=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view the topics with their most important words and their proportions\n",
    "ldamodel.print_topics(num_topics=num_topics, num_words=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For understanding the LDA plot:\n",
    "    \n",
    "- click a circle in the left panel to select a topic.\n",
    "- the bar chart in the right panel will display the 30 most relevant terms for the selected topic.\n",
    "- the red bars represent the frequency of a term in a given topic, (proportional to p(term | topic)). \n",
    "- the blue bars represent a term's frequency across the entire corpus, (proportional to p(term)). \n",
    "- small values of λ (near 0) highlight potentially rare, but exclusive terms for the selected topic. \n",
    "- large values of λ (near 1) highlight frequent, but not necessarily exclusive, terms for the selected topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the intractive LDA plot\n",
    "lda_display = pyLDAvis.gensim.prepare(ldamodel, \n",
    "                                      doc_term_matrix, \n",
    "                                      dictionary, \n",
    "                                      sort_topics=False)\n",
    "pyLDAvis.display(lda_display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the plot in html format\n",
    "pyLDAvis.save_html(lda_display, f\"{output_path}/covid_topic_{num_topics}.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA as feature\n",
    "\n",
    "Here we get the dominant topic and its proportion per document, and concatenate them with the main dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user inputs\n",
    "corpus = doc_term_matrix\n",
    "texts = articles\n",
    "df = articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get dominant topic, percentage of contribution, and keywords for each document\n",
    "def format_topics_sentences(ldamodel, corpus):\n",
    "\n",
    "    results = []\n",
    "    \n",
    "    # get main topic in each document\n",
    "    for row in ldamodel[corpus]:\n",
    "        \n",
    "        if len(row) == 0:\n",
    "            continue\n",
    "            \n",
    "        row = list(sorted(row, key=lambda elem: elem[1], reverse=True))\n",
    "        \n",
    "        # get the dominant topic, percentage of contribution and keywords for each document\n",
    "        topic_num, prop_topic = row[0]        \n",
    "        wp = ldamodel.show_topic(topic_num)\n",
    "        topic_keywords = \", \".join([word for word, prop in wp])\n",
    "        results.append((topic_num, round(prop_topic, 4), [topic_keywords]))\n",
    "    \n",
    "    df = pd.DataFrame.from_records(results, columns=['dominant_topic', 'weight', 'keywords'])\n",
    "    \n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topics = format_topics_sentences(ldamodel, corpus)\n",
    "df_topics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate with the main dataset\n",
    "articles = pd.concat([articles, df_topics.reindex(articles.index)], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map to topic labels\n",
    "\n",
    "Here we map the topic labels to the `dominant_topic` column obtained above. The topic labels are defined by analysing the LDA interactive plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the topic labels for all the topics identified.\n",
    " \n",
    "topics_dict = [[0, 'label_1'],\n",
    "               [1, 'label_2'], \n",
    "               [2, 'label_3'], \n",
    "               [3, 'label_4'],\n",
    "               [4, 'label_5'],\n",
    "               [5, 'label_6']]\n",
    "\n",
    "labels = pd.DataFrame(topics_dict, columns =['topic_num', 'topic_label'])\n",
    "\n",
    "# merge with the main dataset\n",
    "articles = pd.merge(articles, labels, how='left', left_on = 'dominant_topic', right_on='topic_num')\n",
    "articles.drop(\"topic_num\", axis=1, inplace=True)\n",
    "articles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data in data assets\n",
    "articles.to_csv(f\"{output_path}/ws_2_article_topic_{num_topics}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Authors\n",
    "    \n",
    "* **Mehrnoosh Vahdat** is Data Scientist with Data Science & AI Elite team where she specializes in Data Science, Analytics platforms, and Machine Learning solutions.\n",
    "* **Vincent Nelis** is Senior Data Scientist with Data Science & AI Elite team where he specializes in Data Science, Analytics platforms, and Machine Learning solutions.\n",
    "* **Anthony Ayanwale** is Data Scientist with CPAT team where he specializes in Data Science, Analytics platforms, and Machine Learning solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright © IBM Corp. 2020. Licensed under the Apache License, Version 2.0. Released as licensed Sample Materials."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
