{"cells":[{"metadata":{"collapsed":true},"cell_type":"markdown","source":["## Topic Modelling"]},{"metadata":{},"cell_type":"markdown","source":["The goal of this notebook is to find the topics on which people are talking within our dataset with tweets about vaccines. There are many models available for topic modelling, but in this Notebook we've focused only on **LDA (Latent Dirichlet Allocation)**.\n","\n","For data protection purposes, the dataset used in this notebook is not provided here. If you want to replicate the notebook using this dataset, please contact the authors.\n","\n","\n","#### Input\n","- A dataset with tweets ready to be used by our LDA algorithm: `vacc_proc_for_topicMdl.csv`\n","\n","#### Output\n","- An html where we can visualise the discovered topics: `Vaccs_Notts_topic_7.html`\n","- A dataset with tweets mapped to their main topic: `topics_mapped_Vaccs_Notts.csv`"]},{"metadata":{},"cell_type":"code","source":["# ----------------------------------------\n","# Libraries need to be installed\n","# ----------------------------------------\n","\n","!pip install pyLDAvis\n","!pip install gensim\n","!pip install spacy\n","!python -m spacy download en_core_web_sm\n","\n","\n","# ----------------------------------------    \n","# For File operations\n","# ----------------------------------------\n","\n","import zipfile\n","import os\n","\n","# ----------------------------------------\n","# Data read, write and other operations on Texts\n","# ----------------------------------------\n","\n","import pandas as pd\n","import numpy as np\n","import string\n","import re\n","import unicodedata\n","from pprint import pprint\n","\n","# ----------------------------------------\n","# For Libaries for NLP applications\n","# ----------------------------------------\n","\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.util import ngrams\n","from nltk.tokenize import word_tokenize\n","from nltk.probability import FreqDist\n","import gensim\n","import spacy\n","spcy = spacy.load('/opt/conda/envs/Python-3.6-WMLCE/lib/python3.6/site-packages/en_core_web_sm/en_core_web_sm-2.3.1')\n","from gensim import corpora\n","from gensim.models import CoherenceModel\n","\n","# ----------------------------------------\n","# For ignoring some warnings\n","# ----------------------------------------\n","\n","import warnings\n","warnings.filterwarnings('ignore')\n","def wrng():\n","  warnings.warn(\"deprecated\", DeprecationWarning)\n","\n","with warnings.catch_warnings():\n","  warnings.simplefilter(\"ignore\")\n","  wrng()\n","\n","# ----------------------------------------    \n","# For Visualizations\n","# ----------------------------------------\n","\n","import matplotlib\n","import matplotlib.pyplot as plt\n","import pyLDAvis\n","import pyLDAvis.gensim as pygen\n","pyLDAvis.enable_notebook()\n","\n","# ----------------------------------------    \n","# Need to download some extras\n","# ----------------------------------------\n","\n","nltk.download('punkt')\n","nltk.download('stopwords')"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["### Load Dataset\n","Here the datset is used from the `TopicModelling_Vaccine_Preprocessing` notebook."]},{"metadata":{},"cell_type":"code","source":["processed_tweets_Vaccs_ = pd.read_csv(\"/project_data/data_asset/vacc_proc_for_topicMdl.csv\")\n","pd.set_option('display.max_columns', None)  # Showing all columns for that dataframe"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["### Filtering data related to 'Nottingham' "]},{"metadata":{},"cell_type":"code","source":["notts_tweets_Vaccs_ = processed_tweets_Vaccs_[processed_tweets_Vaccs_[\"City\"] == \"Nottingham\"]"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["### Part-of-Speech tagging\n"," Filtering words based on particular part-of-speech as other parts of speech could generate noise for topics"]},{"metadata":{},"cell_type":"code","source":["sentences = []\n","for line in notts_tweets_Vaccs_[\"Clean_sentence_Comment\"]:\n","    pos_ = spcy(line)\n","\n","    sentence2 = \" \".join([token.text for token in pos_ if (token.pos_ == \"ADJ\" or token.pos_ == \"NOUN\" or token.pos_ == \"PROPN\" or token.pos_ == \"VERB\")])\n","    sentences.append(sentence2)\n","    \n","notts_tweets_Vaccs_[\"Clean_sentence_Comment\"] = sentences"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["### Filtering words\n","Filtering the least and most frequent words (filters if less than 'no_below', more than 'no_above')"]},{"metadata":{},"cell_type":"code","source":["words = [text.split() for text in notts_tweets_Vaccs_[\"Clean_sentence_Comment\"]]\n","dict_words = corpora.Dictionary(words)\n","dict_words.filter_extremes(no_below=5, no_above=0.2) \n","dict_words.compactify()\n","myCorpus_notts = [dict_words.doc2bow(word) for word in words]"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["### Training LDA Model\n","Here we train the LDA model and compute the coherence metric and log-perplexity for a range of topic numbers and other hyperparameters. Here, we've focused on coherence metric to choose the best model."]},{"metadata":{},"cell_type":"code","source":["\n","MulLda_coherent_scores = []\n","MulLda_topics_val = []\n","MulLda_perplexity_val = []\n","alpha_val = [0.05, 0.1, 0.3, 0.5, 0.8, 1]\n","MulLda_alphas = []\n","\n","\n","for topics in range(3, 15, 2):\n","        for alph in alpha_val:\n","\n","            lda_model_multi_notts = gensim.models.LdaMulticore(corpus = myCorpus_notts,\n","                         id2word = dict_words,\n","                         random_state = 42,\n","                         num_topics = topics,\n","                         passes=10,\n","                         chunksize=512,\n","                         alpha=alph,\n","                         offset=64,\n","                         eta=None,\n","                         iterations=100,\n","                         per_word_topics=True,\n","                         workers=6)\n","  \n","            coherence_model_MulLda_notts = CoherenceModel(model = lda_model_multi_notts, \n","                                       texts = words, \n","                                       dictionary = dict_words, \n","                                       coherence = 'c_v')\n","  \n","            coherence_MulLda = coherence_model_MulLda_notts.get_coherence()\n","            perplexity_MulLda = lda_model_multi_notts.log_perplexity(myCorpus_notts)\n","        \n","            MulLda_topics_val.append(topics)\n","            MulLda_alphas.append(alph)        \n","            MulLda_coherent_scores.append(coherence_MulLda)\n","            MulLda_perplexity_val.append(perplexity_MulLda)\n","            \n","\n","            \n","df_mulLDA_notts = pd.DataFrame(list(zip(MulLda_topics_val, MulLda_alphas, MulLda_coherent_scores, MulLda_perplexity_val)), \n","                         columns = [\"MulLda_Topic_Num\", \"MulLda_alpha_val\", \"MulLda_Coherent_score\", \"MulLda_Perplexity_val\"])\n","\n","df_mulLDA_notts.sort_values(\"MulLda_Coherent_score\", axis = 0, ascending = False, \n","                     inplace = True) \n","\n","df_mulLDA_notts.head()"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["### Final Model\n","After choosing best hyperparams from above dataframe based on coherence metric, we can train our final model. Note that we haven't just fully relied on the highest value for this metric, but we have rather chosen the model that makes the most sense based in our experience from the top models.   \n","\n","The cell below will output the words related to some topics and clusters of topics (visualization)."]},{"metadata":{},"cell_type":"code","source":["\n","multi_lda_final_notts = gensim.models.LdaMulticore(corpus = myCorpus_notts,\n","                         id2word = dict_words,\n","                         random_state = 42,\n","                         num_topics = 7,\n","                         passes=10,\n","                         chunksize=512,\n","                         alpha=0.05,\n","                         offset=64,\n","                         eta=None,\n","                         iterations=100,\n","                         per_word_topics=True,\n","                         workers=6)\n","\n","pprint(multi_lda_final_notts.print_topics(num_topics = 7, num_words=20))\n","\n","print(\"\\n\\033[91m\" + \"\\033[1m\" +\"------- Visualization -----------\\n\")\n","\n","lda_Mul_vis_notts = pygen.prepare(multi_lda_final_notts, myCorpus_notts, dict_words)\n","pyLDAvis.display(lda_Mul_vis_notts)\n"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["### Saving Topics as html"]},{"metadata":{},"cell_type":"code","source":["pyLDAvis.save_html(lda_Mul_vis_notts, \"/project_data/data_asset/Vaccs_Notts_topic_7.html\")"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["### Mapping Tweets with Topics"]},{"metadata":{},"cell_type":"code","source":["\n","topicss = []\n","probss = []\n","\n","for i, row in enumerate(multi_lda_final_notts[myCorpus_notts]):     # gives topics probablity\n","\n","    row = sorted(row[0], key=lambda x :(x[1]), reverse=True)    # sorting according to higher probability\n","    for j, (topic_num, probablity) in enumerate(row):        # j=0  --> containing highest probablity, topic_num --> falls under which topic\n","        if j == 0:\n","            topicss.append(topic_num)\n","            probss.append(probablity)\n","            \n","Notts_tweets_Vaccs_[\"Topic_Num\"] = topicss\n","Notts_tweets_Vaccs_[\"Topic_prob\"] = probss\n","\n","Notts_tweets_Vaccs_.head()\n","\n"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["### Final Dataset\n","we've given the topics some names and mapped with the tweets"]},{"metadata":{},"cell_type":"code","source":["\"\"\"\n","\n","list_ - values of list needs to converted to string\n","\n","\"\"\"\n","def ListToStr(list_):\n","  str_val = \"\"\n","  for item in list_:\n","    str_val += item\n","  return str_val\n","\n","dts = []\n","\n","for dttt in Notts_tweets_Vaccs_[\"Date\"]:\n","    yrs_ = re.findall(r\"\\d{4}\", dttt)\n","    dts.append(ListToStr(yrs_))\n","    \n","Notts_tweets_Vaccs_[\"year\"] = dts\n","\n","Notts_tweets_Vaccs_[\"Date\"] = pd.to_datetime(Notts_tweets_Vaccs_[\"Date\"]).dt.date\n","\n","tpc_nms = []\n","\n","for tpc_ in Notts_tweets_Vaccs_[\"Topic_Num\"].values.tolist():\n","    if tpc_ == 0:\n","        tpc_nms.append(\"Effects of virus and vaccine\")\n","    if tpc_ == 1:\n","        tpc_nms.append(\"Politics in US around vaccine\")\n","    if tpc_ == 2:\n","        tpc_nms.append(\"Enforcement of vaccines\")\n","    if tpc_ == 3:\n","        tpc_nms.append(\"Politics in UK around vaccine\")\n","    if tpc_ == 4:\n","        tpc_nms.append(\"Science around Vaccine\")\n","    if tpc_ == 5:\n","        tpc_nms.append(\"Public affairs\")\n","    if tpc_ == 6:\n","        tpc_nms.append(\"Distribution of vaccine and logistics\")\n","        \n","Notts_tweets_Vaccs_[\"Topic_Names\"] = tpc_nms\n","\n","\n","tyms = []\n","\n","for tym in Notts_tweets_Vaccs_[\"Date\"].values.tolist():\n","    tym_ = tym.strftime('%d-%b')\n","    tyms.append(tym_)\n","    \n","Notts_tweets_Vaccs_[\"Date_month\"] = tyms\n"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["### Saving Final dataset"]},{"metadata":{},"cell_type":"code","source":["Notts_tweets_Vaccs_.to_csv('/project_data/data_asset/topics_mapped_Vaccs_Notts.csv', index = False)"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["  \n","  \n","### Author:"]},{"metadata":{},"cell_type":"markdown","source":["-  **Ananda Pal** is a Data Scientist and Performance Test Analyst at IBM, where he specialises in Data Science and Machine Learning Solutions"]},{"metadata":{},"cell_type":"markdown","source":["Copyright © IBM Corp. 2020. Licensed under the Apache License, Version 2.0. Released as licensed Sample Materials."]},{"metadata":{},"cell_type":"code","source":[],"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3.6","language":"python"},"language_info":{"name":"python","version":"3.6.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}