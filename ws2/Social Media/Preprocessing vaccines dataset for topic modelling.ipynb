{"cells": [{"metadata": {}, "cell_type": "markdown", "source": "## Data Pre-processing for Topic Modelling"}, {"metadata": {}, "cell_type": "markdown", "source": "In this notebook, we pre-process our data a bit further in order to make it ready for topic modelling.\n\nFor data protection purposes, the dataset that is used in this notebook is not provided here. If you want to replicate the analysis on this dataset, please contact the authors. \n\n#### Input\n- Our processed Meltwater dataset: `Meltwater_processed.csv`\n\n#### Output\n- A slightly more processed dataset that's suitable for topic modelling: `vacc_proc_for_topicMdl.csv`"}, {"metadata": {}, "cell_type": "markdown", "source": "### Importing Necessary Libraries"}, {"metadata": {"collapsed": true}, "cell_type": "code", "source": "# ----------------------------------------\n# Libraries need to be installed\n# ----------------------------------------\n\n!pip install TextBlob\n!pip install gensim\n!pip install spacy\n!python -m spacy download en_core_web_sm\n\n\n# ----------------------------------------    \n# For File operations\n# ----------------------------------------\n\nimport zipfile\n\n# ----------------------------------------\n# Data read, write and other operations on Texts\n# ----------------------------------------\n\nimport pandas as pd\nimport numpy as np\nimport string\nimport re\nimport unicodedata\nfrom pprint import pprint\n\n# ----------------------------------------\n# For Libaries for NLP applications\n# ----------------------------------------\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\nfrom nltk.tokenize import word_tokenize\nfrom nltk.probability import FreqDist\nimport gensim\nimport spacy\nspcy = spacy.load('/opt/conda/envs/Python-3.6-WMLCE/lib/python3.6/site-packages/en_core_web_sm/en_core_web_sm-2.3.1')\nfrom gensim import corpora\nfrom gensim.models import CoherenceModel\nfrom textblob import TextBlob\n\n# ----------------------------------------\n# For ignoring some warnings\n# ----------------------------------------\n\nimport warnings\nwarnings.filterwarnings('ignore')\ndef wrng():\n  warnings.warn(\"deprecated\", DeprecationWarning)\n\nwith warnings.catch_warnings():\n  warnings.simplefilter(\"ignore\")\n  wrng()\n\n\n# ----------------------------------------    \n# Need to download some extras\n# ----------------------------------------\n\nnltk.download('punkt')\nnltk.download('stopwords')", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Reading the data\n"}, {"metadata": {}, "cell_type": "code", "source": "Unproc_df = pd.read_csv(\"/project_data/data_asset/Meltwater_processed.csv\")\npd.set_option('display.max_columns', None)  # Showing all columns for that dataframe", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Checking for Blank Tweets"}, {"metadata": {}, "cell_type": "code", "source": "print(\"\\033[94m\" + \"\\033[1m\" + \"Before Removing Null Tweets Total length of Data is - \", len(Unproc_df))\n\nif Unproc_df[\"Clean text_original text\"].isin([np.nan]).any() == True:                                             #Removing Rows if NULL Tweets exists from Processed\n        Unproc_df = Unproc_df.dropna(subset=[\"Clean text_original text\"], axis = 0).reset_index(drop=True)\n        \nif Unproc_df[\"Clean text_comment\"].isin([np.nan]).any() == True:                                             #Removing Rows if NULL Tweets exists from Processed\n        Unproc_df = Unproc_df.dropna(subset=[\"Clean text_comment\"], axis = 0).reset_index(drop=True)\n        \nprint(\"\\033[94m\" + \"\\033[1m\" + \"After Removing Null Tweets Total length of Data is - \", len(Unproc_df))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Appending 'User Comments' and 'Original Tweet'\n\nFrom our Pre-processing dataset, there're two different coloumns for user-comments and Original tweets. Merging both will help in topic modelling."}, {"metadata": {}, "cell_type": "code", "source": "sntnccmts__ = []\n\n\nfor qt_, txt_, cmt_ in zip(Unproc_df[\"Is QT\"], Unproc_df[\"Clean text_original text\"], Unproc_df[\"Clean text_comment\"]):\n    if qt_ == 0:\n        sntnccmts__.append(txt_)\n    else:\n        sntnccmts__.append(txt_ + \" \" + cmt_)\n        \nUnproc_df[\"Clean_sentence_Comment\"] = sntnccmts__", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Cleaning Junk-words\n\n Tweets contain lots of junk words (like 'looool', 'haahaa', '5555666' etc.) that are nothing but noise. Here, we:\n- First, fetch all junk words and verify whether we are missing any important information.\n- Second, add them to our stopwords list and remove them from our corpus."}, {"metadata": {}, "cell_type": "code", "source": "# ----------------------------------------    \n# Here, finding and checking junk-words\n# ----------------------------------------\n\n\"\"\"\n\ndf__  -->  dataframe to be modified\n\n\"\"\"\n\ndef repeats_(df__):\n\n    repeats_ = []\n\n    for sent_ in df__[\"Clean_sentence_Comment\"].tolist():\n        for ww in sent_.split():\n            pattern = re.findall(r'(.)\\1\\1+', ww)\n            if (str(pattern).strip('[]'))!= \"\":\n                repeats_.append(ww)\n        \n    print(\"\\033[94m\" + \"\\033[1m\" + \"Total count of repetitive words for dataset is - \", len(set(repeats_)))        \n    print(\"\\033[94m\" + \"\\033[1m\" + \"Some repetitative words for dataset are:\\n\" + \"\\033[0m\", repeats_[0:20])\n    \n    return repeats_\n\n\n\"\"\"\n\nsent  -->  sentence to be modified\n\n\"\"\"\n\ndef tokenize(sent):\n    final_tokens = word_tokenize(sent)\n    return final_tokens\n\n\n# ----------------------------------------    \n# Now, Removing Junk-words\n# ----------------------------------------\n\n\n\"\"\"\n\ndf__  -->  dataframe to be modified\nrepeats__  -->  extra stopwords found from above cell\n\n\"\"\"\n\ndef del_excess_(df__, repeats__):\n    \n    stop_words_ext = stopwords.words('english')\n    stop_words_ext.extend(set(repeats__))             # Adding set() here, otherwise we are adding some words more than once\n\n    sentences = []\n    for line in df__[\"Clean_sentence_Comment\"].values.tolist():\n        tokenized_word = tokenize(line)\n        words = [w for w in tokenized_word if len(w) > 2 and w not in stop_words_ext]           # Removing stopwords and words having length <= 2\n        sentence = \" \".join(words)\n        sentences.append(sentence.strip())\n\n    df__[\"Clean_sentence_Comment\"] = sentences\n    \n    return df__\n\n\n\n_repeats_ = repeats_(Unproc_df)\n\nUnproc_df = del_excess_(Unproc_df, _repeats_)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Lemmatization\n\nThere're different forms of one root word that have same meaning. This root word is called _lemma_. For instance, the lemma for the word 'best' is 'good'. Condensing words into their lemmatised forms can be very helpful in topic modelling."}, {"metadata": {}, "cell_type": "code", "source": "\n\"\"\"\n\ndf__  -->  The dataframe needs to modified\n\n\"\"\"\n\n\ndef lemm_(df__):\n\n\n    # Lemmetization Begins\n\n    all_lemmas = []\n    \n    for tweet in df__[\"Clean_sentence_Comment\"]:\n        sentence = spcy(tweet)\n        text = ' '.join([elem.lemma_ for elem in sentence])\n        all_lemmas.append(text)\n    \n    df__[\"Clean_sentence_Comment\"] = all_lemmas\n\n    return df__\n\n\nprocessed_tweets_Vaccs_ = lemm_(Unproc_df)\n\nprint(\"\\033[94m\" + \"\\033[1m\" + \"Before Removing Null Tweets Total length of Data is - \", len(processed_tweets_Vaccs_))\n\n# -----------------------------------\n#Removing Rows if NULL Tweets exists from Processed\n# -----------------------------------\n\nif processed_tweets_Vaccs_[\"Clean_sentence_Comment\"].isin([np.nan]).any() == True:                                             \n        processed_tweets_Vaccs_ = processed_tweets_Vaccs_.dropna(subset=[\"Clean_sentence_Comment\"], axis = 0).reset_index(drop=True)\n\n        \nprint(\"\\033[94m\" + \"\\033[1m\" + \"After Removing Null Tweets Total length of Data is - \", len(processed_tweets_Vaccs_))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Saving Final Dataset"}, {"metadata": {}, "cell_type": "code", "source": "processed_tweets_Vaccs_.to_csv('/project_data/data_asset/vacc_proc_for_topicMdl.csv', index = False)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "  \n  \n### Author:"}, {"metadata": {}, "cell_type": "markdown", "source": "-  **Ananda Pal** is a Data Scientist and Performance Test Analyst at IBM, where he specialises in Data Science and Machine Learning Solutions"}, {"metadata": {}, "cell_type": "markdown", "source": "Copyright \u00a9 IBM Corp. 2020. Licensed under the Apache License, Version 2.0. Released as licensed Sample Materials."}, {"metadata": {}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.6", "language": "python"}, "language_info": {"name": "python", "version": "3.6.10", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 1}