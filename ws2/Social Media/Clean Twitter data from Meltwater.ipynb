{"cells": [{"metadata": {"collapsed": true}, "cell_type": "markdown", "source": "# Cleaning Meltwater data\n\nIn this notebook, we clean and extract information from the text field of the Twitter data extracted from Meltwater. \n\nAs of 30/11, the data consists of a sample collected for the period from 01/10/2020 to 29/11/2020. Tweets are only from the cities of Nottingham and Liverpool and are based on the following keyword query: `vaccin* OR vax OR vaxx OR vaxxx OR jab OR Pfizer OR AstraZeneca OR (Astra NEAR/1 Zeneca) OR Moderna OR antivac* OR antivax* OR anti-vac* OR anti-vax* OR (anti NEAR/1 vac*) OR immun*`.\n\nFor data protection purposes, the dataset that is used in this notebook is not provided here. If you want to replicate the analysis on this dataset, please contact the authors. \n\n#### Input\n- Dataset with tweets from Nottingham: `Vaccines_search_Nottingham.csv`\n- Datasets with tweets from Liverpool: `Vaccines_search_Liverpool_1.csv` and `Vaccines_search_Liverpool_1.csv`\n\n\n#### Output\n- Unprocessed (but still combined) dataset: `Meltwater_unprocessed.csv`\n- Processed dataset: `Meltwater_processed.csv`"}, {"metadata": {}, "cell_type": "markdown", "source": "### 1. Preliminaries\n\nHere we don't really touch the text in a meaningful way\n"}, {"metadata": {}, "cell_type": "markdown", "source": "#### 1.1. Import packages and data"}, {"metadata": {}, "cell_type": "code", "source": "!pip install gensim\n!pip install spacy\n!python -m spacy download en_core_web_sm", "execution_count": 1, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "import zipfile\nimport os\nimport pandas as pd\nimport numpy as np\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\nimport string\nfrom nltk.tokenize import word_tokenize\nfrom nltk.probability import FreqDist\nimport re\nimport gensim\nfrom gensim import corpora\nfrom pprint import pprint\nimport spacy\nspcy = spacy.load('/opt/conda/envs/Python-3.6-WMLCE/lib/python3.6/site-packages/en_core_web_sm/en_core_web_sm-2.3.1')\n\nimport warnings\nwarnings.filterwarnings('ignore')\ndef wrng():\n  warnings.warn(\"deprecated\", DeprecationWarning)\n\nwith warnings.catch_warnings():\n  warnings.simplefilter(\"ignore\")\n  wrng()\n\nnltk.download('punkt')\nnltk.download('stopwords')\n", "execution_count": 2, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Nottingham data\ndf_notts = pd.read_csv('/project_data/data_asset/Vaccines_search_Nottingham.csv')\nprint('Lenght of dataset:', len(df_notts))", "execution_count": 3, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Liverpool data\ndf_liv1 = pd.read_csv('/project_data/data_asset/Vaccines_search_Liverpool_1.csv')\ndf_liv2 = pd.read_csv('/project_data/data_asset/Vaccines_search_Liverpool_2.csv')\n\nprint('Lengh of dataset:', len(df_liv1), '+', len(df_liv2))", "execution_count": 6, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "df = df_notts.append(df_liv1)\ndf = df.append(df_liv2)\nprint('Lengh of combined dataset:', len(df))", "execution_count": 7, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "#### 1.2. Remove duplicates"}, {"metadata": {}, "cell_type": "code", "source": "def del_duplicate(df__):\n    '''\n    df__  -->  dataframe to be modified\n    '''    \n    print(\"\\033[94m\" + \"\\033[1m\" + \"Original length of data:\", len(df__))\n    df__.drop_duplicates(subset = ['Hit Sentence', 'Influencer', 'Date'], keep = 'first', inplace = True) # Remove if there are duplicates tweets in terms of date, text and author\n    print(\"\\033[94m\" + \"\\033[1m\" + \"After Removing Duplicates Total length of data is - \", len(df__))\n\n\n    if df__[\"Hit Sentence\"].isin([np.nan]).any() == True:                                             #Removing Rows if NULL Tweets exists from Processed\n        df__ = df__.dropna(subset=[\"Hit Sentence\"], axis = 0).reset_index(drop=True)\n        \n    print(\"\\033[94m\" + \"\\033[1m\" + \"After Removing Null Tweets Total length of data is - \", len(df__))\n    \n    return df__\n\n\ndf_unproc =  del_duplicate(df)", "execution_count": 8, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Save unprocessed data\ndf_unproc.to_csv('/project_data/data_asset/Meltwater_unprocessed', index = False)", "execution_count": 9, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 2. Process data\n\nHere we do process text and produce a \"clean\" version. We can use the clean text to do sentiment analysis and topic modelling. \n\nAt the end of this point, we will get two \"clean\" text fields: `Clean text_original text` and `Clean text_comment`. The distinction is necessary due to quoted tweets (QT). A quoted tweet has the following structure: `QT @user: <comment> : <original text>`. Here, our influencer (the person whose tweet we are seeing, in Meltwater's terminology) is quoting a tweet originally published by `@user`, who posted `<original text>`. In her publication, the influencer is adding her `<comment>`. Hence `Clean text_original text` refers to the tweet being quoted, whilst `Clean text_comment` is the comment by the influencer. \n\nIf the tweet is not a QT, the two columns should be the same."}, {"metadata": {}, "cell_type": "markdown", "source": "#### 2.1. Remove RT/QT indicator"}, {"metadata": {}, "cell_type": "code", "source": "# Dummy variables for whether or not tweet is RT (retweet) or QT (quoted tweet)\ndf_unproc['Is RT'] = df_unproc['Hit Sentence'].str.find('RT @', 0, 4)\ndf_unproc['Is RT'].replace(to_replace={0:1, -1:0}, inplace = True)\n\ndf_unproc['Is QT'] = df_unproc['Hit Sentence'].str.find('QT @', 0, 4)\ndf_unproc['Is QT'].replace(to_replace={0:1, -1:0}, inplace = True)", "execution_count": 10, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "def extract_RT_QT(_df):\n    _df['Text_original text'] = _df['Hit Sentence'].str.replace(r'RT @[A-Za-z0-9_-]*: ', '') # Eliminates the RT identifier --> RT @someone:\n    _df['Text_original text'] = _df['Text_original text'].str.replace(r'QT @*[A-Za-z0-9 \\s\\S]* ; ', '') # Eliminates the QT identifier and the comment, leaving only the original tweet --> QT @someone: <comment>\n\n    _df['Text_comment'] = _df['Hit Sentence'].str.replace(r'RT @[A-Za-z0-9_-]*: ', '') # Eliminates the RT identifier --> RT @someone:\n    _df['Text_comment'] = _df['Text_comment'].str.replace(r'QT @[A-Za-z0-9_-]*: ', '') # Eliminates the QT identifier and the original tweet, leaving only the comment from the user (this line and the one below)\n    _df['Text_comment'][_df['Is QT'] == 1] = _df['Text_comment'].str.split(r' ; ', expand = True).iloc[:,0]\n    \n    return _df", "execution_count": 11, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "df_proc = extract_RT_QT(df_unproc)", "execution_count": 12, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "#### 2.2. Clean text\n\n"}, {"metadata": {}, "cell_type": "code", "source": "def process(dframe_):\n    '''\n    dframe_  -->  dataframe to be processed\n    '''\n    def del_mention(sent):\n        '''\n        Removing all the mentions (@...) from dataset\n        '''\n        final_sent = re.sub(\"@[^\\s]+\", \"\", sent).replace(\"  \", \" \")\n        return final_sent\n    \n    def del_hash(sent):\n        '''\n        Removing all the Hash-tags (#...) from dataset\n        '''\n        final_sent = re.sub(\"#[^\\s]+\", \"\", sent).replace(\"  \", \" \")\n        return final_sent\n\n    def del_url(sent):\n        '''\n        Removing all the URLs (http / https...) from dataset\n        '''\n        final_sent = re.sub(r'http\\S+', '', sent).replace(\"  \", \" \")\n        return final_sent\n\n    def tokenize(sent):\n        '''\n        Tokenizing sentences \n        '''\n        final_tokens = word_tokenize(sent)\n        return final_tokens\n\n    def del_punct(sent):\n        '''\n        Removing all punctuations from dataset\n        '''\n        final_sent = re.sub(r'[^\\w\\s]', ' ', sent).replace(\"  \", \" \")\n        final_sent = re.sub(r'\\_', ' ', final_sent).replace(\"  \", \" \")    \n        return final_sent\n    \n    \n    \"\"\"\n    # This will convert chars like \u00d4\u0178\u00f3\u00c9\u017b\u00e2\u00d9\u00db\u00e9.... and remove chars with different languages\n\n    def del_accnt(sent):\n        final_sent = unicodedata.normalize('NFD', sent).encode('ascii', 'ignore').decode(\"utf-8\")\n        return final_sent\n    \"\"\"\n\n    def del_lang_emoji_spclChar(sent):\n        '''\n        Removing all the special characters, emojis, words of different languages from dataset\n        '''\n        final_sent = re.sub(\"[^a-zA-Z0-9 \\t\\n\\r\\f\\v]\", \"\", sent).replace(\"  \", \" \")\n        return final_sent\n\n\n    stop_words = (stopwords.words(\"english\"))\n\n\n    for text_var in ['_original text', '_comment']:\n        sentences = []\n        for line in dframe_[\"Text\" + text_var].values.tolist():\n            sentence = line.lower().replace(\"\\n\", \"\")                                           # Remove \"\\n\" in sentence\n            sentence = del_mention(sentence)                                   \n            sentence = del_hash(sentence)                                    \n            sentence = del_url(sentence) \n            sentence = del_punct(sentence)\n            tokenized_word = tokenize(sentence)                                      \n            words = [w for w in tokenized_word if len(w) > 2 and w not in stop_words]           # Removing stopwords and words having length <= 2\n            sentence2 = \" \".join(words)\n            sentence2 = del_lang_emoji_spclChar(sentence2)                                      # Removing words from different language, punctuations, special chars and emojis\n            tokenized_word2 = tokenize(sentence2)                                      \n            words2 = [w2 for w2 in tokenized_word2 if len(w2) > 2]                              # words having length <= 2 again\n            sentence2 = \" \".join(words2)  \n            sentences.append(sentence2.strip())\n        \n        dframe_[\"Clean text\" + text_var] = sentences\n\n    return dframe_\n\n", "execution_count": 167, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "%%time\n# Running the above function\ndf_proc = process(df_proc)\ndf_proc.head(10)", "execution_count": 13, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Check there are no empty values in our text variable after processing\nassert df_proc['Clean text_original text'].isnull().any() == False\nassert df_proc['Clean text_comment'].isnull().any() == False\n\nassert df_proc['Text_original text'].isnull().any() == False\nassert df_proc['Text_comment'].isnull().any() == False", "execution_count": 14, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Save processed data\ndf_proc.to_csv('/project_data/data_asset/Meltwater_processed.csv', index = False)", "execution_count": 15, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "________\n\n#### Authors\n- **\u00c1lvaro Corrales Cano** is a Data Scientist within IBM's Cloud Pak Acceleration team. With a background in Economics, \u00c1lvaro specialises in a wide array Econometric techniques and causal inference, including regression, discrete choice models, time series and duration analysis.\n- **Ananda Pal** is a Data Scientist and Performance Test Analyst at IBM, where he specialises in Data Science and Machine Learning Solutions. \n\nCopyright \u00a9 IBM Corp. 2020. Licensed under the Apache License, Version 2.0. Released as licensed Sample Materials.\n"}, {"metadata": {}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.6", "language": "python"}, "language_info": {"name": "python", "version": "3.6.10", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 1}