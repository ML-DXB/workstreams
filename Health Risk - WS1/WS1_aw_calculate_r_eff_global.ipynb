{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Calculation of the global R_eff numbers\n",
    "## Objective\n",
    "To be able to understand and later foresee government decisions on lockdown measures, we need to look at the same metrics as governments do. Among others as active cases or intensive care units, this is also the (effective) reproduction number.\n",
    "\n",
    "\n",
    "## Data Sources\n",
    "### Patients Linelist\n",
    "Global linelist from beoutbreakprepared <https://github.com/beoutbreakprepared/nCoV2019/tree/8b8a11a93f99a2814a809ab9b6a7bc0f94db576a/latest_data> with data catalogue here <https://www.nature.com/articles/s41597-020-0448-0> \n",
    "\n",
    "The data is a tar.gz (Status 2020-05-29)\n",
    "### Casenumbers\n",
    "Johns Hopkins global casenumbers <https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/>.\n",
    "The raw datasource is preprocessed to get one pivoted dataframe. \n",
    "\n",
    "\n",
    "## Calculation Procedure\n",
    "The calculation of R_eff consists of multiple calculation\n",
    "### Create a linelist\n",
    "In the global linelist from beoutbreakprepared, there are not all cases listed, so in order to impute symptoms onset dates for the missing datapoints, we need a list, which inherits these gaps. This list can be created from a linelist from Johns Hopkins Data, sa here are all cases listed and substract the patients linelist for the specific country from beoutbreakprepared.\n",
    "### Imputation\n",
    "At first the datapoints with missing symptoms-onset date are imputed. Therefor for all datapoints for which we have the reporting and the symptoms onset date the reporting delay is calculated. Then a Weibull distribution is fitted for the reporting delay. This distribution is then applied to the datapoints with missing symptoms onset date for assigning a certain reporting delay to these datapoints and calculate the symptons onset date.\n",
    "### Nowcasting\n",
    "The same distribution is then used to adapt the casenumbers to account for possible cases which will be reported in the future with symptoms onset date until today.\n",
    "### Rolling Window Ratio\n",
    "The calculation of r_eff itself is performed as a summation of casenumbers within a certain timeperiod. We use 4 days (according to RKI for Germany). Then the ratio of two sums is compared. Here we also compare the sums with 4 days inbetween (according to RKI for Germany).\n",
    "### Validation\n",
    "The calculation procedure was validated for Germany and compared with the official published numbers from RKI for nowcasted and r_eff values.\n",
    "<<https://www.rki.de/DE/Content/InfAZ/N/Neuartiges_Coronavirus/Projekte_RKI/Nowcasting_Zahlen.html>>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code toggling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "import random\n",
    "\n",
    "def hide_toggle(for_next=False):\n",
    "    this_cell = \"\"\"$('div.cell.code_cell.rendered.selected')\"\"\"\n",
    "    next_cell = this_cell + '.next()'\n",
    "\n",
    "    toggle_text = 'Toggle show/hide'  # text shown on toggle link\n",
    "    target_cell = this_cell  # target cell to control with toggle\n",
    "    js_hide_current = ''  # bit of JS to permanently hide code in current cell (only when toggling next cell)\n",
    "\n",
    "    if for_next:\n",
    "        target_cell = next_cell\n",
    "        toggle_text += ' next cell'\n",
    "        js_hide_current = this_cell + '.find(\"div.input\").hide();'\n",
    "\n",
    "    js_f_name = 'code_toggle_{}'.format(str(random.randint(1,2**64)))\n",
    "\n",
    "    html = \"\"\"\n",
    "        <script>\n",
    "            function {f_name}() {{\n",
    "                {cell_selector}.find('div.input').toggle();\n",
    "            }}\n",
    "\n",
    "            {js_hide_current}\n",
    "        </script>\n",
    "\n",
    "        <a href=\"javascript:{f_name}()\">{toggle_text}</a>\n",
    "    \"\"\".format(\n",
    "        f_name=js_f_name,\n",
    "        cell_selector=target_cell,\n",
    "        js_hide_current=js_hide_current, \n",
    "        toggle_text=toggle_text\n",
    "    )\n",
    "\n",
    "    return HTML(html)\n",
    "\n",
    "hide_toggle()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import weibull_min \n",
    "from datetime import date \n",
    "import ipywidgets as widgets\n",
    "import difflib\n",
    "import random\n",
    "from itertools import repeat\n",
    "\n",
    "hide_toggle()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global patients linelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patients=pd.read_csv(\"../latestdata_20200720.csv\",\n",
    "                                             parse_dates=False,\n",
    "    usecols=[\n",
    "        'country',\n",
    "        'date_confirmation',\n",
    "        'date_onset_symptoms'],\n",
    "    low_memory=False)\n",
    "\n",
    "hide_toggle()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enrich the global linelist data with countrycode to match the global casenumbers.\n",
    "The mapping table can also be found in this git repo <<https://github.com/rolls-royce/EMER2GENT/blob/master/data/sun/geo/country_name_mapping.csv>>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfMapping=pd.read_csv(\"../country_name_mapping.csv\")\n",
    "for country in patients.country.unique():\n",
    "    try:\n",
    "        ADM0_A3 = dfMapping[dfMapping.name == country].ADM0_A3.values[0]\n",
    "        patients.loc[patients[\"country\"]==country,\"ADM0_A3\"]=ADM0_A3\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "hide_toggle()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Datetime Formatting\n",
    "2. Filter the global linelist for all rows, which have both: reporting and symptoms onset date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patients.date_confirmation=pd.to_datetime(patients.date_confirmation,format=\"%d.%m.%Y\",errors=\"coerce\")\n",
    "patients.date_onset_symptoms=pd.to_datetime(patients.date_onset_symptoms,format=\"%d.%m.%Y\",errors=\"coerce\")\n",
    "patients=patients.dropna(axis=0,how=\"any\")\n",
    "\n",
    "hide_toggle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the patients linelist to a parquet file for faster processing in the next run:\n",
    "patients.to_parquet(\"../patients_line_list.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import linelist!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patients=pd.read_parquet(\"../patients_line_list.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patients.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global casenumbers\n",
    "\n",
    "Preprocessing of the raw data from Johns Hopkins University. Here just a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cssegi_confirmed=pd.read_parquet(\"../time_series_covid19_confirmed_global.parquet\")\n",
    "df_cssegi_deaths=pd.read_parquet(\"../time_series_covid19_deaths_global.parquet\")\n",
    "df_cssegi_recovered=pd.read_parquet(\"../time_series_covid19_recovered_global.parquet\")\n",
    "# Add Countrycodes\n",
    "all_countries = df_cssegi_confirmed[\"Country/Region\"]\n",
    "ADM0_A3=[]\n",
    "ISO_3_code_i=[]\n",
    "for country in all_countries:\n",
    "    ADM0_A3.append(dfMapping[dfMapping.name == country].ADM0_A3.values[0])\n",
    "    ISO_3_code_i.append(dfMapping[dfMapping.name == country].ISO_3_code_i.values[0])   \n",
    "df_cssegi_confirmed[\"ADM0_A3\"]=ADM0_A3\n",
    "df_cssegi_confirmed[\"ISO_3_code_i\"]=ISO_3_code_i\n",
    "df_cssegi_confirmed.sort_index(axis=1, inplace=True,ascending=False)\n",
    "\n",
    "all_countries = df_cssegi_deaths[\"Country/Region\"]\n",
    "ADM0_A3=[]\n",
    "ISO_3_code_i=[]\n",
    "for country in all_countries:\n",
    "    ADM0_A3.append(dfMapping[dfMapping.name == country].ADM0_A3.values[0])\n",
    "    ISO_3_code_i.append(dfMapping[dfMapping.name == country].ISO_3_code_i.values[0])\n",
    "df_cssegi_deaths[\"ADM0_A3\"]=ADM0_A3\n",
    "df_cssegi_deaths[\"ISO_3_code_i\"]=ISO_3_code_i\n",
    "df_cssegi_deaths.sort_index(axis=1, inplace=True,ascending=False)\n",
    "\n",
    "all_countries = df_cssegi_recovered[\"Country/Region\"]\n",
    "ADM0_A3=[]\n",
    "ISO_3_code_i=[]\n",
    "for country in all_countries:\n",
    "    ADM0_A3.append(dfMapping[dfMapping.name == country].ADM0_A3.values[0])\n",
    "    ISO_3_code_i.append(dfMapping[dfMapping.name == country].ISO_3_code_i.values[0])\n",
    "df_cssegi_recovered[\"ADM0_A3\"]=ADM0_A3\n",
    "df_cssegi_recovered[\"ISO_3_code_i\"]=ISO_3_code_i\n",
    "df_cssegi_recovered.sort_index(axis=1, inplace=True,ascending=False)\n",
    "\n",
    "# Pivoting\n",
    "val_cols = list(df_cssegi_confirmed.columns)\n",
    "\n",
    "df_cssegi_confirmed_pivot=pd.melt(df_cssegi_confirmed, id_vars=['Province/State', 'Country/Region', 'Lat', 'Long','ADM0_A3','ISO_3_code_i'], \n",
    "        value_vars=val_cols[6:], value_name=\"ConfirmedCases\", var_name=\"Date\").drop(columns=['Lat', 'Long'])\n",
    "df_cssegi_deaths_pivot=pd.melt(df_cssegi_deaths, id_vars=['Province/State', 'Country/Region', 'Lat', 'Long','ADM0_A3','ISO_3_code_i'], \n",
    "        value_vars=val_cols[6:], value_name=\"FatalityCases\", var_name=\"Date\").drop(columns=['Lat', 'Long'])\n",
    "df_cssegi_recovered_pivot=pd.melt(df_cssegi_recovered, id_vars=['Province/State', 'Country/Region', 'Lat', 'Long','ADM0_A3','ISO_3_code_i'], \n",
    "        value_vars=val_cols[6:], value_name=\"RecoveredCases\", var_name=\"Date\").drop(columns=['Lat', 'Long'])\n",
    "\n",
    "df_cssegi=pd.merge(df_cssegi_confirmed_pivot,df_cssegi_deaths_pivot, on = ['Province/State', 'Country/Region','Date','ADM0_A3','ISO_3_code_i'])\n",
    "df_cssegi=pd.merge(df_cssegi,df_cssegi_recovered_pivot, on = ['Province/State', 'Country/Region','Date','ADM0_A3','ISO_3_code_i'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or read the preprocessed file directly:\n",
    "\n",
    "df_cssegi=pd.read_csv(\"../johns_hopkins_casenumbers_all_countries.csv\")\n",
    "\n",
    "hide_toggle()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just use the countrycodes which are present in both datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list1=list(patients.ADM0_A3.unique())\n",
    "list2=list(df_cssegi.ADM0_A3.unique())\n",
    "\n",
    "country_codes=list(set(list1).intersection(list2))\n",
    "print(\"global linelist no countries: \", len(list1), \"Johns hopkins casenumbers no countries: \", len(list2), \"\", len(country_codes))\n",
    "\n",
    "hide_toggle()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Routine for creating a multiple options clickbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Routine for multicheckbox from github\n",
    "\n",
    "def multi_checkbox_widget(descriptions):\n",
    "    \"\"\" Widget with a search field and lots of checkboxes \"\"\"\n",
    "    search_widget = widgets.Text()\n",
    "    options_dict = {description: widgets.Checkbox(description=description, value=False) for description in descriptions}\n",
    "    options = [options_dict[description] for description in descriptions]\n",
    "    options_widget = widgets.VBox(options, layout={'overflow': 'scroll'})\n",
    "    multi_select = widgets.VBox([search_widget, options_widget])\n",
    "\n",
    "    # Wire the search field to the checkboxes\n",
    "    def on_text_change(change):\n",
    "        search_input = change['new']\n",
    "        if search_input == '':\n",
    "            # Reset search field\n",
    "            new_options = [options_dict[description] for description in descriptions]\n",
    "        else:\n",
    "            # Filter by search field using difflib.\n",
    "            close_matches = difflib.get_close_matches(search_input, descriptions, cutoff=0.0)\n",
    "            new_options = [options_dict[description] for description in close_matches]\n",
    "        options_widget.children = new_options\n",
    "\n",
    "    search_widget.observe(on_text_change, names='value')\n",
    "    return multi_select\n",
    "\n",
    "hide_toggle()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Routine for padding the timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Routine for padding the timeseries\n",
    "\n",
    "idx = pd.IndexSlice\n",
    "\n",
    "date_index=pd.date_range(start='01/01/2020',end=date.today(),freq='D')\n",
    "\n",
    "def filldates(df):\n",
    "    df.reset_index(drop=True,inplace=True)\n",
    "    df=df.asfreq(\"1D\",fill_value=0.0).reindex(date_index,fill_value=0.0)\n",
    "    return df\n",
    "\n",
    "hide_toggle()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Routine to create a linelist from the Johns Hopkins Data \n",
    "Also integrate the dates with symptoms onset date from the patients data, if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Routine to create a linelist from the Johns Hopkins Data \n",
    "\n",
    "#l = []\n",
    "#l.extend(repeat(x, 100))\n",
    "def create_linelist(df_cssegi,df_patients_onset):\n",
    "    HELP=[]\n",
    "    #HELP2=[]\n",
    "    for index,row in df_cssegi.iterrows():\n",
    "        HELP.extend(repeat(row[\"Date\"],row[\"ConfirmedCases\"]))\n",
    "\n",
    "    HELP=pd.to_datetime(HELP)\n",
    "    #HELP2=pd.to_datetime(HELP2)\n",
    "    df_linelist=pd.DataFrame(columns=[\"ReportingDate\",\"SymptomsOnsetDate\"],data=list(zip(HELP,HELP)))\n",
    "\n",
    "    # list with only valid dates, delay > 0 days \n",
    "    # use the patients linelist for this\n",
    "    df_linelist_clean=pd.DataFrame({\"ReportingDate\":pd.to_datetime(df_patients_onset[\"date_confirmation\"]),\n",
    "                                    \"SymptomsOnsetDate\":pd.to_datetime(df_patients_onset[\"date_onset_symptoms\"])})\n",
    "    \n",
    "    # Include the delay of 0 days for cases where we do not have the symptoms onset date.\n",
    "    df_linelist.loc[:,\"delay\"]=df_linelist[\"ReportingDate\"]-df_linelist[\"SymptomsOnsetDate\"]\n",
    "\n",
    "    # just for the cases where we do have both dates\n",
    "    df_linelist_clean.loc[:,\"delay\"]=df_linelist_clean[\"ReportingDate\"]-df_linelist_clean[\"SymptomsOnsetDate\"]\n",
    "    \n",
    "    # Now integrate the linelist_clean dates into the other linelist\n",
    "    # This is quite timeconsuming!\n",
    "    #for i in df_linelist_clean.itertuples():\n",
    "    #    for j in df_linelist.itertuples():\n",
    "    #        if ((i[1]==j[1])&(j[3].days==0)):\n",
    "    #            df_linelist.loc[j[0],\"SymptomsOnsetDate\"]=i[2]\n",
    "    #            df_linelist.loc[j[0],\"delay\"]=i[3]\n",
    "    #            break\n",
    "      \n",
    "    \n",
    "    for i in df_linelist_clean.itertuples():\n",
    "        idx=((df_linelist[\"ReportingDate\"]==i[1])&(df_linelist[\"SymptomsOnsetDate\"]==i[1])).idxmax\n",
    "        df_linelist.loc[idx,\"SymptomsOnsetDate\"]=i[2]\n",
    "        df_linelist.loc[idx,\"delay\"]=i[3]\n",
    "    \n",
    "    return df_linelist, df_linelist_clean\n",
    "\n",
    "hide_toggle()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Routine for performing the imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Routine for performing the imputation \n",
    "# Fitting Weibull Distribution\n",
    "def imputation(df_linelist,df_linelist_clean,name):\n",
    "    shape, loc, scale = weibull_min.fit(df_linelist_clean[\"delay\"].dt.days, floc=0)\n",
    "    wb_row=[name,shape,loc,scale]\n",
    "    \n",
    "# distribute missing values accordingly to that distribution:\n",
    "# create random numbers and assign to the missing delay values in the dataframe:\n",
    "# overall values\n",
    "    size=len(df_linelist)\n",
    "    r=weibull_min.rvs(shape,loc=loc,scale=scale,size=size)\n",
    "    df_linelist.loc[:,\"delay_weibull\"]=[datetime.timedelta(days=int(i)) for i in r]\n",
    "    df_linelist.loc[:,\"SymptomsOnsetDate Weibull\"]=df_linelist[\"ReportingDate\"]-df_linelist[\"delay_weibull\"]\n",
    "    #print(np.mean(df_linelist[\"delay_weibull\"]))\n",
    "\n",
    "# combine the known values with the imputated values:\n",
    "    df_linelist.loc[df_linelist[\"delay\"].dt.days<=0,\"Combined\"]=df_linelist.loc[df_linelist[\"delay\"].dt.days<=0,\"SymptomsOnsetDate Weibull\"]\n",
    "    df_linelist.loc[df_linelist[\"delay\"].dt.days>0,\"Combined\"]=df_linelist.loc[df_linelist[\"delay\"].dt.days>0,\"SymptomsOnsetDate\"]\n",
    "\n",
    "    onset_combined=df_linelist[\"Combined\"].value_counts()\n",
    "    \n",
    "# padding the timeseries data to get an entry for every day\n",
    "    onset_combined.sort_index(inplace=True)\n",
    "    onset_combined=onset_combined.asfreq(\"1D\",fill_value=0.0).reindex(date_index,fill_value=0.0)\n",
    "    \n",
    "    number=max(df_linelist[\"delay\"].dt.days)\n",
    "    \n",
    "# calculate p_delay for nowcasting routine\n",
    "    p_delay=pd.Series(data=weibull_min.pdf(np.arange(0,number),shape,loc=loc,scale=scale),index=np.arange(0,number))\n",
    "  \n",
    "    return onset_combined,p_delay,wb_row\n",
    "\n",
    "hide_toggle()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a global linelist delay distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the global linelist delay distribution\n",
    "df_linelist_clean_global=patients\n",
    "print(\"Globally we have \",len(df_linelist_clean_global),\" cases available with symptoms onset date.\")\n",
    "df_linelist_clean_global.loc[:,\"delay\"]=df_linelist_clean_global[\"date_confirmation\"]-df_linelist_clean_global[\"date_onset_symptoms\"]\n",
    "# fitting a Weibull distribution to the global delay\n",
    "shape_global, loc_global, scale_global = weibull_min.fit(df_linelist_clean_global[\"delay\"].dt.days, floc=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the global delay distribution and the fitted Weibull curve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.hist(df_linelist_clean_global[\"delay\"].dt.days, density=True, alpha=0.5,bins=50,color=\"blue\")\n",
    "x = np.linspace(df_linelist_clean_global[\"delay\"].dt.days.min(), df_linelist_clean_global[\"delay\"].dt.days.max(), 100) \n",
    "plt.plot(x, weibull_min(shape_global, loc=loc_global, scale=scale_global).pdf(x),color=\"black\") \n",
    "plt.title(\"Histogram and fitted Weibull distribution for the global reporting delay\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Routine for performing the imputation based on the global delay distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imputation_global(df_linelist,df_linelist_clean,name):\n",
    "# distribute missing values accordingly to that distribution:\n",
    "# create random numbers and assign to the missing delay values in the dataframe:\n",
    "# overall values\n",
    "    size=len(df_linelist)\n",
    "    r=weibull_min.rvs(shape_global,loc=loc_global,scale=scale_global,size=size)\n",
    "    wb_row=[name,shape_global,loc_global,scale_global]\n",
    "        \n",
    "    df_linelist.loc[:,\"delay_weibull\"]=[datetime.timedelta(days=int(i)) for i in r]\n",
    "    df_linelist.loc[:,\"SymptomsOnsetDate Weibull\"]=df_linelist[\"ReportingDate\"]-df_linelist[\"delay_weibull\"]\n",
    "    \n",
    "    # combine the known values with the imputated values:\n",
    "    df_linelist.loc[df_linelist[\"delay\"].dt.days<=0,\"Combined\"]=df_linelist.loc[df_linelist[\"delay\"].dt.days<=0,\"SymptomsOnsetDate Weibull\"]\n",
    "    df_linelist.loc[df_linelist[\"delay\"].dt.days>0,\"Combined\"]=df_linelist.loc[df_linelist[\"delay\"].dt.days>0,\"SymptomsOnsetDate\"]\n",
    "\n",
    "    onset_combined=df_linelist[\"Combined\"].value_counts()\n",
    "    \n",
    "# padding the timeseries data to get an entry for every day\n",
    "    onset_combined.sort_index(inplace=True)\n",
    "    onset_combined=onset_combined.asfreq(\"1D\",fill_value=0.0).reindex(date_index,fill_value=0.0)\n",
    "    \n",
    "    number=max(df_linelist[\"delay\"].dt.days)\n",
    "    \n",
    "# calculate p_delay for nowcasting routine\n",
    "    p_delay=pd.Series(data=weibull_min.pdf(np.arange(0,number),shape_global,loc=loc_global,scale=scale_global),index=np.arange(0,number))\n",
    "    \n",
    "    return onset_combined, p_delay, wb_row\n",
    "\n",
    "hide_toggle()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nowcasting Routine from rt.live"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nowcasting Routine from rt.live\n",
    "\n",
    "def adjust_onset_for_right_censorship(onset, p_delay):\n",
    "    cumulative_p_delay = p_delay.cumsum()\n",
    "    \n",
    "    # Calculate the additional ones needed so shapes match\n",
    "    ones_needed = len(onset) - len(cumulative_p_delay)\n",
    "    padding_shape = (0, ones_needed)\n",
    "    \n",
    "    # Add ones and flip back\n",
    "    cumulative_p_delay = np.pad(\n",
    "        cumulative_p_delay,\n",
    "        padding_shape,\n",
    "        mode=\"constant\",\n",
    "        constant_values=1)\n",
    "    cumulative_p_delay = np.flip(cumulative_p_delay)\n",
    "    \n",
    "    # Adjusts observed onset values to expected terminal onset values\n",
    "    adjusted = onset / cumulative_p_delay\n",
    "    \n",
    "    return adjusted, cumulative_p_delay\n",
    "\n",
    "hide_toggle()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which timewindow to use for the r_eff calculation? (RKI uses 4 days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the timewindow for sliding window\n",
    "# RKI uses 7 days:\n",
    "window=widgets.IntSlider(\n",
    "    value=7,\n",
    "    min=0,\n",
    "    max=10,\n",
    "    step=1,\n",
    "    description='Sliding time window',\n",
    "    disabled=False,\n",
    "    continuous_update=False,\n",
    "    orientation='horizontal',\n",
    "    readout=True,\n",
    "    readout_format='d'\n",
    ")\n",
    "\n",
    "display(window)\n",
    "\n",
    "#hide_toggle()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Routine for performing the r_eff calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Routine for performing the r_eff calculation\n",
    "def calculate_reff(onset_adjusted):\n",
    "    df_rolling=onset_adjusted.rolling(window.value).sum()\n",
    "    r_t=df_rolling.pct_change(periods=window.value)+1.0\n",
    "    r_t.sort_index(inplace=True)\n",
    "    return r_t\n",
    "\n",
    "hide_toggle()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grouping the overall casenumbers dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_cssegi=df_cssegi.groupby(\"ADM0_A3\")\n",
    "\n",
    "hide_toggle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def save_obj(obj, name):\n",
    "    with open('../r_eff_numbers_globally/'+ name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name ):\n",
    "    with open('../r_eff_numbers_globally/' + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "    \n",
    "hide_toggle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform the calculation for all available countries and save the results to dataframes\n",
    "### Perform the calculation (Imputation, Nowcasting, r_eff)\n",
    "If there are no cases with symptoms onset data for a specific country or if the percentage of cases with symptoms onset date is below a certain threshold (10%?) we will use a global delay distribution, which is calculated from all given symptoms onset dates. That is very bad approximation, but is used as a start. (This is also the approach used by rt.live\n",
    "For each country a linelist is created from Johns Hopkins numbers and then combined with the patients linelist from beoutbreakprepared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the complete calculation for all countries:\n",
    "\n",
    "max_count=len(country_codes)\n",
    "progress=widgets.IntProgress(min=0,max=max_count)\n",
    "#display(progress)\n",
    "\n",
    "data=[]\n",
    "imputed=pd.DataFrame(index=date_index)\n",
    "nowcast=pd.DataFrame(index=date_index)\n",
    "r_eff=pd.DataFrame(index=date_index)\n",
    "\n",
    "#for code in country_codes:\n",
    "for code in [\"BEL\",\"FRA\",\"ITA\",\"GBR\"]:\n",
    "    print(\"code = \",code)\n",
    "    name=dfMapping[dfMapping[\"ADM0_A3\"]==code].name.values[0]\n",
    "    print(\"name = \",name)   \n",
    "    # get cssegi data\n",
    "    df=grouped_cssegi.get_group(code)\n",
    "    \n",
    "    # get patients data\n",
    "    df_patients=patients[patients[\"country\"]==name]\n",
    "    \n",
    "    # create linelist\n",
    "    df_linelist,df_linelist_clean=create_linelist(df, df_patients)\n",
    "    print(\"linelist done\")\n",
    "    \n",
    "    # get share of data with symptoms onset date\n",
    "    df.set_index(\"date\", inplace=True)\n",
    "    df.index=pd.to_datetime(df.index)\n",
    "    df.sort_index(inplace=True)    \n",
    "    # How big is the share of patients with symptons onset date given?\n",
    "    # --> can we use the country specific delay distribution or do we use the global delay distribution?\n",
    "    FLAG_GLOBAL=False\n",
    "    share_symptoms=len(df_patients)/df[\"confirmed\"][-1]*100\n",
    "    if share_symptoms < 10:\n",
    "        print(\"the global delay distribution will be used for country \", name)\n",
    "        FLAG_GLOBAL=True\n",
    "                   \n",
    "    # perform imputation\n",
    "    # Call the routine for imputing the missing dates Depending on the flag FLAG_GLOBAL, we either use the delay distribution of the specific country or use the global distribution.\n",
    "    if FLAG_GLOBAL==False:\n",
    "        onset_combined,p_delay,wb_row=imputation(df_linelist,df_linelist_clean,name)  \n",
    "    elif FLAG_GLOBAL==True:\n",
    "        onset_combined,p_delay,wb_row=imputation_global(df_linelist,df_linelist_clean,name) \n",
    "    else:\n",
    "        print(\"s.th. went wrong, the FLAG_GLOBAL was not set.\")\n",
    "    print(\"imputation done\")\n",
    "    \n",
    "    # perform nowcasting\n",
    "    onset_adjusted, cumulative_p_delay = adjust_onset_for_right_censorship(onset_combined, p_delay)\n",
    "    print(\"nowcasting done\")\n",
    "    \n",
    "    # calculate r_t\n",
    "    r_t=calculate_reff(onset_adjusted)\n",
    "    print(\"r_t done\")\n",
    "\n",
    "    # convert the Series data into Dataframes and save them for each country as csv\n",
    "    imputed=pd.DataFrame(index=onset_combined.index,data=list(onset_combined),columns=[\"imputed\"])\n",
    "    nowcast=pd.DataFrame(index=onset_adjusted.index,data=list(onset_adjusted),columns=[\"nowcast\"])\n",
    "    r_eff=pd.DataFrame(index=r_t.index,data=list(r_t),columns=[\"r_eff\"])\n",
    "    \n",
    "    # put the information into the file if a country specific or the global delay distribution was used\n",
    "    if FLAG_GLOBAL==False:\n",
    "        flag_global=\"\"\n",
    "    elif FLAG_GLOBAL==True:\n",
    "        flag_global=\"global_delay_distr\"\n",
    "    \n",
    "    imputed.to_csv(\"../r_eff_numbers_globally/imputed\"+name+flag_global+\".csv\",index_label=False)\n",
    "    nowcast.to_csv(\"../r_eff_numbers_globally/nowcast\"+name+flag_global+\".csv\",index_label=False)\n",
    "    r_eff.to_csv(\"../r_eff_numbers_globally/r_eff\"+name+flag_global+\".csv\",index_label=False)\n",
    "    \n",
    "    # add the dataframes to an \"all data\" array and save that one in the end.\n",
    "    #\n",
    "    \n",
    "    wb_row.append(FLAG_GLOBAL)\n",
    "    data.append(wb_row)\n",
    "                   \n",
    "    progress.value+=1\n",
    "    \n",
    "# save the curve parameters to a dataframe    \n",
    "df_wb=pd.DataFrame(columns=[\"country\",\"shape\",\"loc\",\"scale\",\"flag_global\"],data=data)\n",
    "df_wb=df_wb.set_index(\"country\")\n",
    "\n",
    "hide_toggle()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the \"all data\" array to a dataframe and then to csv or pkl."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save_obj(nowcast,\"nowcast\")\n",
    "#nowcast=load_obj(\"nowcast\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save_obj(r_eff,\"r_eff\")\n",
    "#r_eff=load_obj(\"r_eff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save_obj(df_wb,\"df_wb\")\n",
    "#r_eff=load_obj(\"df_wb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run only for selected countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options=dfMapping[dfMapping.ADM0_A3.isin(country_codes)].name.unique()\n",
    "select_plot = multi_checkbox_widget(options)\n",
    "\n",
    "select_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important: Please just select one country!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for automated job run: preselect one country\n",
    "#name=\"Australia\"\n",
    "\n",
    "# extract the name from the selection\n",
    "name = [w.description for w in select_plot.children[1].children if w.value]\n",
    "print(\"Selected Country: \",name)\n",
    "# give warning if more than one country is selected\n",
    "if len(name)>1:\n",
    "    print(len(name), \"Please select only one country!\")\n",
    "# get the country_code from the country name\n",
    "country_code=dfMapping[dfMapping[\"name\"]==name[0]].ADM0_A3.values[0]\n",
    "print(\"Selected Country Code: \",country_code)\n",
    "# get the overall casenumbers for the selected country\n",
    "df=grouped_cssegi.get_group(country_code)\n",
    "df_raw=df.copy()\n",
    "#plt.figure\n",
    "df.set_index(\"date\", inplace=True)\n",
    "df.index=pd.to_datetime(df.index)\n",
    "df.sort_index(inplace=True)\n",
    "print(\"confirmed cases: \",df[\"confirmed\"][-1])\n",
    "\n",
    "df_patients=patients[patients[\"country\"]==name[0]]\n",
    "print(\"number of patients in linelist with 'symptoms onset' and 'confirmation' date: \",len(df_patients))\n",
    "print(\"that is \",len(df_patients)/df[\"confirmed\"][-1]*100,\"% of all cases\")\n",
    "\n",
    "# Create linelist for the selected country from Johns Hopkins data and combine it with the patients data.\n",
    "df_linelist,df_linelist_clean=create_linelist(df_raw, df_patients)\n",
    "\n",
    "# How big is the share of patients with symptons onset date given?\n",
    "# --> can we use the country specific delay distribution or do we use the global delay distribution?\n",
    "FLAG_GLOBAL=False\n",
    "share_symptoms=len(df_patients)/df[\"confirmed\"][-1]*100\n",
    "if share_symptoms < 10:\n",
    "    print(\"the global delay distribution will be used for country \", name[0])\n",
    "    FLAG_GLOBAL=True\n",
    "    \n",
    "# Call the routine for imputing the missing dates Depending on the flag FLAG_GLOBAL, we either use the delay distribution of the specific country or use the global distribution.\n",
    "if FLAG_GLOBAL==False:\n",
    "    onset_combined,p_delay,wb_row=imputation(df_linelist,df_linelist_clean,name[0])  \n",
    "elif FLAG_GLOBAL==True:\n",
    "    onset_combined,p_delay,wb_row=imputation_global(df_linelist,df_linelist_clean,name[0]) \n",
    "else:\n",
    "    print(\"s.th. went wrong, the FLAG_GLOBAL was not set.\")\n",
    "    \n",
    "# Call the routine for nowcasting\n",
    "onset_adjusted, cumulative_p_delay = adjust_onset_for_right_censorship(onset_combined, p_delay)\n",
    "# calculate r_t\n",
    "r_t=calculate_reff(onset_adjusted)\n",
    "\n",
    "r_eff=pd.DataFrame(index=date_index)\n",
    "r_eff[name[0]]=r_t\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "ax.plot(r_eff.index,r_eff[name[0]],label=name[0])\n",
    "ax.hlines(y=1.0,xmin=min(r_eff.index),xmax=max(r_eff.index),lw=1.0)\n",
    "plt.ylim([0,4])\n",
    "plt.legend()\n",
    "plt.title(\"Reproduction number for selected country \")\n",
    "    \n",
    "hide_toggle() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now compar the results for Germany with the previously produced results with RKI numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
