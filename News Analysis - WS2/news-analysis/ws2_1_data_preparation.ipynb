{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook cleans and structure textual data. We use a sample data generated based on sklearn available dataset of “Twenty Newsgroups”. This is only a small set to run the notebooks with, and not suitable for generating results. For more information about this dataset, please see: https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html\n",
    "\n",
    "#### Input:\n",
    "\n",
    "article_sample.csv: \n",
    "\n",
    "This dataset contains all the articles. It is structured in 3 columns: An article ID, an article, and a publication date.\n",
    "\n",
    "#### Output:\n",
    "\n",
    "The code will produce a dataset saved in a file `ws2_1_article_clean.csv`. The dataset is structured in 6 columns: an article ID, an article (original text), a number of words (in the article), a cleaned version of the text, the number of words (in the cleaned text), and a publication date.\n",
    "\n",
    "#### How do we clean the text of the articles?\n",
    "\n",
    "From every article, we remove:\n",
    "- The white spaces at the beginning and at the end of the text\n",
    "- The HTML tags\n",
    "- All the special characters, i.e. all characters that are not a digit nor an alphabetical letter\n",
    "- All the punctuations\n",
    "- All the english stop-words\n",
    "- All the words that are not an adjective, a noun, or an adverb.\n",
    "\n",
    "Then we use lemmatization to turn every word to its base or dictionary form.\n",
    "\n",
    "We also plot the histograms of the number of words before and after the cleaning process, and filter on the article length for noise reduction.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Include all the dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import string\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.options.display.max_colwidth = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The path to the output folder where all the outputs will be saved\n",
    "output_path = \"/project_data/data_asset\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    \"\"\" Removes from the input text:\n",
    "        - html tags, \n",
    "        - punctuations, \n",
    "        - stop words,\n",
    "        - words of less than 3 characters\n",
    "        - all words that are not a noun, an adjective, or a verb.\n",
    "\n",
    "    Arguments:\n",
    "        text (str) :\n",
    "            The text to be cleaned\n",
    "\n",
    "    Raises:\n",
    "        TypeError: if input parameters are not of the expected type\n",
    "\n",
    "    Returns:\n",
    "        text (str) :\n",
    "            The cleaned text (lowercased)\n",
    "    \"\"\"\n",
    "    \n",
    "    if not isinstance(text, str):\n",
    "        raise TypeError(\"Argument 'text' must be a string.\")\n",
    "    \n",
    "    # Strip the text, lowercase it, and remove the HTML tags and punctuations\n",
    "    text = text.lower().strip()\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    text = re.sub(r'[^ 0-9a-z]', ' ', text)\n",
    "    text = re.sub(r'\\b(\\d+\\d)\\b', '', text)\n",
    "    text = re.sub(r'http|https|www', '', text)\n",
    "    text = re.sub(r'\\b[a-z]\\b', '', text)\n",
    "    text = re.sub(r' +', ' ', text)\n",
    "    text = text.translate(text.maketrans('', '', string.punctuation)) #extra punctuations removal\n",
    "\n",
    "    # Remove all the stop words\n",
    "    stop_words = nltk.corpus.stopwords.words('english')\n",
    "    stop_words.extend([\n",
    "        'from', 're', 'also'\n",
    "    ])\n",
    "    stop_words = {key: True for key in set(stop_words)}\n",
    "    \n",
    "    # Keep only specific pos (part of speech: nouns, adjectives, and adverbs)\n",
    "    keep_pos = {key: True for key in ['NN','NNS','NNP','NNPS', 'JJ', 'JJR', 'JJS','RB','RBR','RBS']}\n",
    "    \n",
    "    return \" \".join([word \n",
    "                     for word, pos in nltk.tag.pos_tag(text.split()) \n",
    "                     if len(word) > 2 and word not in stop_words and pos in keep_pos])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(text: str, lemmatizer: nltk.stem.WordNetLemmatizer) -> str:\n",
    "    \"\"\" Lemmatize the words in a sentence by:\n",
    "        - mapping the POS tag to each word,\n",
    "        - lemmatize the word.\n",
    "\n",
    "    Arguments:\n",
    "        sentence (str):\n",
    "            The sentence in which the words need to be lemmatized\n",
    "        lemmatizer:\n",
    "            Lemmatizer function\n",
    "\n",
    "   Raises:\n",
    "        TypeError: if input parameters are not of the expected type\n",
    "\n",
    "    Returns:\n",
    "        Lemmatized text\n",
    "    ----------------------------------------------------------------------------------------\n",
    "    \"\"\"\n",
    "    \n",
    "    if not isinstance(text, str):\n",
    "        raise TypeError(\"Argument 'text' must be a string.\")\n",
    "\n",
    "    lemmas = []\n",
    "    tag_dict = {\n",
    "        \"J\": nltk.corpus.wordnet.ADJ,\n",
    "        \"N\": nltk.corpus.wordnet.NOUN,\n",
    "        \"V\": nltk.corpus.wordnet.VERB,\n",
    "        \"R\": nltk.corpus.wordnet.ADV\n",
    "    }\n",
    "    \n",
    "    tokenized_words = nltk.word_tokenize(text)\n",
    "    for tokenized_word in tokenized_words:\n",
    "        tag = nltk.tag.pos_tag([tokenized_word])[0][1][0].upper() # Map POS tag to first character lemmatize() accepts\n",
    "        wordnet_pos = tag_dict.get(tag, nltk.corpus.wordnet.NOUN)\n",
    "        lemma = lemmatizer.lemmatize(tokenized_word, wordnet_pos)\n",
    "        lemmas.append(lemma)\n",
    "    \n",
    "    return \" \".join(lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the input dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = pd.read_csv(f\"{output_path}/article_sample.csv\")\n",
    "\n",
    "print(f\"Number of records: {len(articles)}\")\n",
    "\n",
    "articles.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse the number of words per article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles[\"n_words\"] = articles[\"article\"].apply(lambda text: len(text.split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# clean article word count\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.hist(articles[articles[\"n_words\"] < 150].n_words, bins = 200, color = [\"#bf7cbb\"])\n",
    "plt.gca().set(xlim=(-10, 150), ylabel='Number of articles', xlabel='Number of words')\n",
    "plt.box(False)\n",
    "plt.title('Number of words per article', fontdict=dict(size=24))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only the articles that contain more than 50 words\n",
    "\n",
    "articles = articles[articles[\"n_words\"] > 50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles[\"article_clean\"] = articles[\"article\"].apply(clean)\n",
    "\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "articles[\"article_clean\"] = articles[\"article_clean\"].apply(lambda x: lemmatize(x, lemmatizer)) \n",
    "\n",
    "articles[\"n_words_clean\"] = articles[\"article_clean\"].apply(lambda x: len(x.split(\" \")))\n",
    "\n",
    "articles.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse the number of words per clean article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# clean article word count\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.hist(articles[articles[\"n_words_clean\"] < 400].n_words_clean, \n",
    "         bins = 200, \n",
    "         color = [\"#bf7cbb\"])\n",
    "plt.xlim((-10, 150))\n",
    "plt.xlabel(\"Article word count\", fontsize=18)\n",
    "plt.ylabel(\"Number of articles\", fontsize=18)\n",
    "plt.box(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles.to_csv(f\"{output_path}/ws2_1_article_clean.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Authors\n",
    "    \n",
    "* **Mehrnoosh Vahdat** is Data Scientist with Data Science & AI Elite team where she specializes in Data Science, Analytics platforms, and Machine Learning solutions.\n",
    "* **Vincent Nelis** is Senior Data Scientist with Data Science & AI Elite team where he specializes in Data Science, Analytics platforms, and Machine Learning solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright © IBM Corp. 2020. Licensed under the Apache License, Version 2.0. Released as licensed Sample Materials."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
