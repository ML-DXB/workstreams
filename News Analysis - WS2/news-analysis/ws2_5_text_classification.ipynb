{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook includes the process of training a binary classification model on textual data. Please note that the notebook runs on a small sample dataset, and the generated results are not usable.\n",
    "\n",
    "#### Input:\n",
    "\n",
    "ws_2_article_topic_XX.csv:\n",
    "This dataset contains the clean text and the LDA results as features obtained from `ws2_2_topic_modelling` notebook where XX is the optimal number of topics. We create a binary label using 'topic_label' of this dataset.\n",
    "\n",
    "#### Output:\n",
    "\n",
    "'covid_classifier' model:\n",
    "This notebook trains a binary classification model using SGDClassifier of sklearn and deploy it in the IBM Cloud Pak for Data deployment space.\n",
    "\n",
    "tf_idf.csv:\n",
    "The code will produce the tf_idf matrix as CSV for training the classification model with AutoAI.\n",
    "\n",
    "\n",
    "#### Classification workflow includes:\n",
    "\n",
    "- Import data\n",
    "- Data split and upsampling\n",
    "- Pipeline of TF-IDF tokenization method and Linear SVM classifier using SGDClassifier\n",
    "- Hyper-parameter tuning with grid search\n",
    "- Model evaluation\n",
    "- Prepare data for AutoAI\n",
    "- Save and deploy model with WML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics, ensemble\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration parameters\n",
    "\n",
    "The data path of the data storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The path to the output folder where all the outputs will be saved\n",
    "output_path = \"/project_data/data_asset\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data\n",
    "\n",
    "Data contains clean articles and topic labels that are generated based on the result of LDA analysis with measuring the coherence metric. Here we import data and create a binary label where `topic_of_interest`is the topic we aim to detect with a classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Article Dataset\n",
    "\n",
    "df_clean = pd.read_csv(f\"{output_path}/ws_2_article_topic_6.csv\")\n",
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the topic of interest for bianry classification\n",
    "topic_of_interest = 'label_5'\n",
    "\n",
    "# create a binary label where 1 is the topic of interest and 0 is the rest\n",
    "df_clean['label'] = np.where(df_clean['topic_label'] == topic_of_interest, 1, 0)  \n",
    "df_clean['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data split and upsampling\n",
    "\n",
    "The dataset is inbalance in the label column, significantly more 0 class than 1 class. Upsampling will allow us increase the proportion of 1 class in data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spliting the data into train and test sets\n",
    "from sklearn import model_selection, preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# cleaned body of documents\n",
    "X = df_clean['article_clean']  \n",
    "# Target variable\n",
    "y = df_clean['label'] \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    stratify=y, \n",
    "                                                    test_size=0.25,\n",
    "                                                    random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upsampling\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# concatenate our training data back together\n",
    "train_concat = pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "# separate minority and majority classes\n",
    "majority_class = train_concat[train_concat.label==0]\n",
    "minority_class = train_concat[train_concat.label==1]\n",
    "\n",
    "# upsample the minority class\n",
    "minority_upsampled = resample(minority_class,\n",
    "                              replace=True, # sample with replacement\n",
    "                              n_samples=len(majority_class), # match number in majority class\n",
    "                              random_state=27) # reproducible results\n",
    "\n",
    "# combine majority and upsampled minority\n",
    "upsampled = pd.concat([majority_class, minority_upsampled])\n",
    "\n",
    "#shuffle \n",
    "upsampled = upsampled.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "y_train = upsampled['label']\n",
    "X_train = upsampled['article_clean']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline\n",
    "\n",
    "Creation of the step by step process that will lead to the classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('vect',TfidfVectorizer(max_df = 0.8, min_df=0.0001, norm = 'l2', use_idf = True)), \n",
    "    ('clf',SGDClassifier(random_state=0, alpha = 2e-05, penalty = 'l2', loss = 'hinge')) \n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-parameter tuning with grid search\n",
    "\n",
    "Finding optimal parameters for the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "parameters = {\n",
    "    'vect__max_df': (0.6, 0.7, 0.8), # max threshold for document frequency\n",
    "    'vect__min_df': (0.0001, 0.001),\n",
    "    'vect__max_features': (None, 5000, 10000), # top max_features ordered by term frequency across the corpus\n",
    "    'vect__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams\n",
    "    'vect__use_idf': (True, False), # Enables inverse-document-frequency reweighting\n",
    "    'vect__norm': (None, 'l1', 'l2'), # normalizes tf-idf in each row\n",
    "    'clf__loss': ('log','modified_huber','hinge'), # log for logistic regression, modified_huber gives a smooth loss tolerant to outliers, hinge for linear SVM\n",
    "    'clf__alpha': (0.00001,0.00002), # multiplies the regularization term\n",
    "    'clf__penalty': ('none','l2','l1'), # regularization term\n",
    "    'clf__max_iter': (10, 50), # passes over the training data (aka epochs)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search with cross validation\n",
    "grid_search = GridSearchCV(pipeline, parameters, cv=5, scoring='roc_auc', n_jobs=10, verbose=1, iid=False) # 'roc_auc' or 'f1'\n",
    "\n",
    "print(\"pipeline:\", [name for name, _ in pipeline.steps])\n",
    "print(\"parameters:\")\n",
    "print(parameters)\n",
    "\n",
    "# model training\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Result of grid search\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)  \n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring the test set\n",
    "\n",
    "Selecting best model from grid search and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best estimator found from grid search\n",
    "model = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model predictions\n",
    "\n",
    "predictions = model.predict(X_test)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model evaluation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return of the Accuracy, Precision, Recall, F1 Score, AUC score\n",
    "\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, predictions))\n",
    "print(\"Precision:\",metrics.precision_score(y_test, predictions))\n",
    "print(\"Recall:\",metrics.recall_score(y_test, predictions))\n",
    "print(\"f1_score:\",metrics.f1_score(y_test, predictions))\n",
    "print('AUC: ', metrics.roc_auc_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix of predictions\n",
    "\n",
    "cnf_matrix = metrics.confusion_matrix(y_test, predictions)\n",
    "cnf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data for AutoAI\n",
    "\n",
    "extracting TF-IDF matrix with 1000 most important words with label to input into AutoAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tf-idf matrix\n",
    "Tvectorizer = TfidfVectorizer(max_df = 0.6, min_df=0.0001, norm = 'l2', max_features=1000)\n",
    "X_tfidf = Tvectorizer.fit_transform(df_clean['article_clean'])\n",
    "# place tf-idf values in a pandas data frame\n",
    "tfidf_df = pd.DataFrame(X_tfidf.todense(), columns=Tvectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add label column to dataframe\n",
    "tfidf_df['label'] = df_clean['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data\n",
    "tfidf_df.to_csv(f\"{output_path}/tf_idf.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and deploy model with WML\n",
    "\n",
    "We save and deploy the model by connecting to the ICP4D local Watson Machine Learning using CP4D credentials. Watson Machine Learning provides deployment spaces where the user can save, configure and deploy their models. We can save models, functions and data assets in this space. The steps involved in saving and deploying the model are detailed in the following cells. We will use the watson_machine_learning_client package to complete these steps.\n",
    "\n",
    "* Connect to WML client\n",
    "* Save the model in the deployment space repository\n",
    "* Deploy the model ONLINE\n",
    "\n",
    "### Connect to WML client\n",
    "\n",
    "We will use the watson_machine_learning_client package to complete these steps. We establish a connection to the Watson Machine Learning API with the system credentials and set the default space and project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from watson_machine_learning_client import WatsonMachineLearningAPIClient\n",
    "import os\n",
    "\n",
    "token = os.environ['USER_ACCESS_TOKEN']\n",
    "\n",
    "wml_credentials = {\n",
    "   \"token\": token,\n",
    "   \"instance_id\" : \"openshift\",\n",
    "   \"url\": os.environ['RUNTIME_ENV_APSX_URL'],\n",
    "   \"version\": \"2.5.0\"\n",
    "}\n",
    "\n",
    "client = WatsonMachineLearningAPIClient(wml_credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a deployment space (only run for the first time)\n",
    "space_details = client.spaces.store(meta_props={client.spaces.ConfigurationMetaNames.NAME: \"text_analysis_space\"}) \n",
    "\n",
    "# get the space uid\n",
    "space_uid = client.spaces.get_uid(space_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set project ID\n",
    "project_uid = os.environ['PROJECT_ID']\n",
    "client.set.default_project(project_uid)\n",
    "client.set.default_space(space_uid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the model in the deployment space repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# give model name\n",
    "model = model\n",
    "model_name = 'covid_classifier'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the model details\n",
    "model_props = {client.repository.ModelMetaNames.NAME: model_name,\n",
    "               client.repository.ModelMetaNames.RUNTIME_UID : \"scikit-learn_0.20-py3.6\",\n",
    "               client.repository.ModelMetaNames.TYPE : \"scikit-learn_0.20\",\n",
    "               }\n",
    "\n",
    "# store model in the deployment space\n",
    "stored_model_details = client.repository.store_model(model=model, meta_props=model_props)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy the model ONLINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deployment metadata of the model\n",
    "meta_props = {\n",
    "    client.deployments.ConfigurationMetaNames.NAME: model_name,\n",
    "    client.deployments.ConfigurationMetaNames.ONLINE: {}\n",
    "}\n",
    "\n",
    "# deploy the model\n",
    "model_uid = stored_model_details[\"metadata\"][\"guid\"]\n",
    "deployment_details = client.deployments.create( artifact_uid=model_uid, meta_props=meta_props)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score the deployed model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = [\"article_clean\"]\n",
    "values = [['virus hero key worker full list classify niamh cavanagh mar update mar boris johnson announce today school shut friday notice child key worker vulnerable kid official list release tomorrow list medical professional education name johnson announcement earlier today credit afp list medical professional include doctor nurse midwife paramedic health worker employment medical health community safety police force fire brigade education teacher worker school pre school supermarket worker delivery driver broadcast johnson announce school whole close friday britain desperately contain coronavirus outbreak right time education secretary gavin williamson promise free school meal everyone likely form supermarket voucher boris nation even slow result draconian measure place week believe step already together announce today already slow spread disease pay story']]\n",
    "\n",
    "scoring_payload = {\n",
    "client.deployments.ScoringMetaNames.INPUT_DATA: [{\n",
    "    \"fields\": fields, \n",
    "    \"values\": values\n",
    "}]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dep_id = client.deployments.get_uid(deployment_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.deployments.score(deployment_id=dep_id,meta_props=scoring_payload)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Authors\n",
    "* **Mehrnoosh Vahdat** is Data Scientist with Data Science & AI Elite team where she specializes in Data Science, Analytics platforms, and Machine Learning solutions.\n",
    "* **Anthony Ayanwale** is Data Scientist with CPAT team where he specializes in Data Science, Analytics platforms, and Machine Learning solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright © IBM Corp. 2020. Licensed under the Apache License, Version 2.0. Released as licensed Sample Materials."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
