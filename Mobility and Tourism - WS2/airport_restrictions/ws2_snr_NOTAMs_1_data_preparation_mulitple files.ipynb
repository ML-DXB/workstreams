{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation of multiple NOTAMs files to create a timeline\n",
    "\n",
    "What is NOTAMS?\n",
    "\n",
    "NOTAM actually stands for Notice To Airmen and is the primary means of disseminating all kinds of information to pilots\n",
    "\n",
    "As NOTAMs contain critical information about aircraft and passenger entry requirements, we use NLP to extract relevant information from these messages. NOTAMs are similar to telegram messages and contain a lot of abbreviations to reduce the length of the message. So before extracting information from these message, the text has to be preprocessed.\n",
    "\n",
    "This notebook deals with cleaning the NOTAMs messages that can be later used for further analysis.\n",
    "\n",
    "**NOTE:**\n",
    "\n",
    "The results in this notebook cannot be directly reproduced. The input data has to be downloaded by the user!\n",
    "\n",
    "We read in the NOTAMS data downloaded from https://www.icao.int/safety/iStars/Pages/API-Data-Service.aspx. The data will not be published on this site and has to be downloaded by each user on their own to carry out the following analysis. In order to download the data, the user must register on https://www.icao.int/safety/iStars/Pages/API-Data-Service.aspx to get a free API key.\n",
    "\n",
    "\n",
    "\n",
    "## Data Sources\n",
    "\n",
    "**Data collection from ICAO website**\n",
    "\n",
    "The following data are collected:\n",
    "\n",
    "1. COVID related NOTAMS from airpspaces (Airspace COVID-19 NOTAMs)\n",
    "\n",
    "Airport NOTAMS also provide information about closure of airports\n",
    "\n",
    "\n",
    "Example of a COVID-19 NOTAM message:\n",
    "\n",
    "    COVID-19: ORDERS OF THE STATE GOVERNMENT OF BRANDENBURG WITH THE AIM OF PREVENTING THE INTRODUCTION OR SPREAD OF INFECTIONS BY SARS-COV-2. ALL PAX ENTERING THE FEDERAL REPUBLIC OF GERMANY AS THEIR FINAL DESTINATION FROM RISK AREAS DIRECT OR VIA TRANSFER(1) MUST STAY IN QUARANTINE FOR 14 DAYS AFTER ARRIVAL AND (2) MUST CONTACT LOCAL HEALTH AUTHORITY OF THEIR FINAL DESTINATION IMMEDIATLY. EXCEPTIONS ARE POSSIBLE IN THE CASE OF A NEGARIVE PCR TEST FOR SARS-COV-2 IN GERMAN AND ENGLISH FOR A MAXIMUM OF 48 HOURS BEFORE ENTRY. THESE REGULATIONS DO NOT APPLY FOR CREW MEMBERS. THE CREW MUST PROVIDE INFORMATION ABOUT THESE REGULATIONS TO ALL PAX INFLIGHT. CREATED: 15 Jun 2020 15:58:00 SOURCE: EUECYIYN\n",
    "\n",
    "\n",
    "**Input**\n",
    "\n",
    "   Downloaded datasets \n",
    "\n",
    "    - all_airspace_covid_notams_xx.csv (Multiple files)\n",
    "\n",
    "**Output**\n",
    "  \n",
    "  Preprocessed datasets- multiple airspace notams files (one per week beginning last week of May 2020)\n",
    "\n",
    "    - valid_airspace_notams_xx.csv\n",
    "\n",
    "where 'xx' corresponds to the date\n",
    "\n",
    "The following steps are carried out in preprocessing the data:\n",
    "\n",
    "1. Extracting NOTAMs from json: As the NOTAMs are stored in json format, the intial step is to extract these messages from the json string\n",
    "\n",
    "\n",
    "2. Cleaning the NOTAMS\n",
    "\n",
    "    * Remove white spaces\n",
    "    * Remove hyperlinks\n",
    "    * Mapping abbreviations to actual words\n",
    "    * Remove foreign text if the phrase \"english text/english version\" is present\n",
    "    * Remove words starting with symbols\n",
    "    * Remove punctuations\n",
    "\n",
    "\n",
    "3. Generating tokens\n",
    "\n",
    "    * Remove numbers\n",
    "    * Lemmatization\n",
    "    * Remove stop words\n",
    "\n",
    "4. The latest file for each week is selected and then the above preprocessing steps are carried out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import spacy\n",
    "except:\n",
    "    !pip install spacy\n",
    "try:\n",
    "    import spacy_langdetect\n",
    "except:\n",
    "    !pip install spacy-langdetect\n",
    "try:\n",
    "    import flair\n",
    "except:\n",
    "    !pip install flair\n",
    "try:\n",
    "    import geonamescache\n",
    "except:\n",
    "    !pip install geonamescache\n",
    "try:\n",
    "    import spacy_fastlang\n",
    "except:    \n",
    "    !pip install spacy_fastlang\n",
    "    #!pip install sense2vec==1.0.0a1\n",
    "try:\n",
    "    import gensim\n",
    "except:\n",
    "    !pip install gensim\n",
    "try:\n",
    "    import wordcloud\n",
    "except:\n",
    "    !pip install wordcloud\n",
    "try:\n",
    "    import nltk\n",
    "except:\n",
    "    !pip install nltk\n",
    "!python -m spacy download en_core_web_sm\n",
    "!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "from collections import Counter, defaultdict,OrderedDict\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "import itertools\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import string\n",
    "\n",
    "from spacy_langdetect import LanguageDetector\n",
    "import plac\n",
    "from spacy.lang.en import English\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.tokens import Doc, Span, Token\n",
    "\n",
    "from langdetect import detect, detect_langs\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Extract NOTAM from json**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_airport_message(df):\n",
    "    apt_dt = dict()\n",
    "    apt_dt['message'] = []\n",
    "    apt_dt['Qcode'] = []\n",
    "    apt_dt['createdDate'] = []\n",
    "    apt_dt['Closed'] = []\n",
    "    apt_dt['airportName'] = []\n",
    "    apt_dt['airportCode'] = []\n",
    "    apt_dt['cityName'] = []\n",
    "    apt_dt['countryCode'] = []\n",
    "    apt_dt['countryName'] = []\n",
    "    apt_dt['latitude'] = []\n",
    "    apt_dt['longitude'] = []\n",
    "    \n",
    "    for idx,row in df.iterrows():\n",
    "        if type(row['notams'])==str:\n",
    "            jsd = json.loads(row['notams'])\n",
    "            for m in jsd['message'].values():\n",
    "                m_ = m.split(\"\\nCREATED: \")[0]\n",
    "                m_ = m_.replace('\\n',' ')\n",
    "                #print(m_)\n",
    "                apt_dt['message'].append(m_)\n",
    "                apt_dt['Closed'].append(row['Closed'])\n",
    "                apt_dt['airportName'].append(row['airportName'])\n",
    "                apt_dt['airportCode'].append(row['airportCode'])\n",
    "                apt_dt['cityName'].append(row['cityName'])\n",
    "                apt_dt['countryCode'].append(row['countryCode'])\n",
    "                apt_dt['countryName'].append(row['countryName'])\n",
    "                apt_dt['latitude'].append(row['latitude'])\n",
    "                apt_dt['longitude'].append(row['longitude'])\n",
    "\n",
    "            for qc in jsd['Qcode'].values():\n",
    "                apt_dt['Qcode'].append(qc)\n",
    "            for cd in jsd['Created'].values():\n",
    "                apt_dt['createdDate'].append(cd)\n",
    "        else:\n",
    "            apt_dt['message'].append(None)\n",
    "            apt_dt['Qcode'].append(None)\n",
    "            apt_dt['createdDate'].append(None)\n",
    "            apt_dt['Closed'].append(row['Closed'])\n",
    "            apt_dt['airportName'].append(row['airportName'])\n",
    "            apt_dt['airportCode'].append(row['airportCode'])\n",
    "            apt_dt['cityName'].append(row['cityName'])\n",
    "            apt_dt['countryCode'].append(row['countryCode'])\n",
    "            apt_dt['countryName'].append(row['countryName'])\n",
    "            apt_dt['latitude'].append(row['latitude'])\n",
    "            apt_dt['longitude'].append(row['longitude'])\n",
    "\n",
    "    apt_df = pd.DataFrame(apt_dt)\n",
    "    apt_df['createdDate'] = pd.to_datetime(apt_df['createdDate'])\n",
    "    return apt_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_airspace_message(df):\n",
    "    apt_dt = dict()\n",
    "    apt_dt['message'] = []\n",
    "    apt_dt['Qcode'] = []\n",
    "    apt_dt['createdDate'] = []\n",
    "    apt_dt['Closed'] = []\n",
    "    apt_dt['FIRcode'] = []\n",
    "    apt_dt['FIRname'] = []\n",
    "    apt_dt['countryCode'] = []\n",
    "    apt_dt['countryName'] = []\n",
    "    \n",
    "    for idx,row in df.iterrows():\n",
    "        if type(row['notams'])==str:\n",
    "            jsd = json.loads(row['notams'])\n",
    "            for m in jsd['message'].values():\n",
    "                m_ = m.split(\"\\nCREATED: \")[0]\n",
    "                m_ = m_.replace('\\n',' ')\n",
    "                #print(m_)\n",
    "                apt_dt['message'].append(m_)\n",
    "                apt_dt['FIRname'].append(row['FIRname'])\n",
    "                apt_dt['FIRcode'].append(row['FIRcode'])\n",
    "                apt_dt['countryCode'].append(row['countryCode'])\n",
    "                apt_dt['countryName'].append(row['countryName'])\n",
    "\n",
    "            for qc in jsd['Qcode'].values():\n",
    "                apt_dt['Qcode'].append(qc)\n",
    "            for cd in jsd['Created'].values():\n",
    "                apt_dt['createdDate'].append(cd)\n",
    "            for c_ in jsd['Closed'].values():\n",
    "                apt_dt['Closed'].append(c_)\n",
    "        else:\n",
    "            apt_dt['message'].append(None)\n",
    "            apt_dt['Qcode'].append(None)\n",
    "            apt_dt['createdDate'].append(None)\n",
    "            apt_dt['Closed'].append(None)\n",
    "            apt_dt['FIRname'].append(row['FIRname'])\n",
    "            apt_dt['FIRcode'].append(row['FIRcode'])\n",
    "            apt_dt['countryCode'].append(row['countryCode'])\n",
    "            apt_dt['countryName'].append(row['countryName'])\n",
    "\n",
    "    apt_df = pd.DataFrame(apt_dt)\n",
    "    apt_df['createdDate'] = pd.to_datetime(apt_df['createdDate'])\n",
    "    return apt_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The following API gives a consolidated NOTAMS of Airport and Airspace Restrictions related to COVID 19**\n",
    "\n",
    "List of NOTAMS for airspaces and aairports referring to COVID-19 restrictions\n",
    "\n",
    "Data dictionary of Airport COVID-19 NOTAMS\n",
    "\n",
    "|Field|\tType|\tDescription|\n",
    "|-----|-----|-----|\n",
    "|countryName|\tstring|\tName of the Country|\n",
    "|countryCode|\tstring|\tISO 3-Letter Code of the Country|\n",
    "|airportName|\tstring|\tName of the airport, searchable|\n",
    "|cityName|\tstring|\tName of the city, searchable|\n",
    "|airportCode|\tstring|\tICAO 4-letter code of the airport|\n",
    "|latitude|\tnumber|\tLatitude in Decimal degrees|\n",
    "|longitude|\tnumber|\tLongitude in Decimal degrees|\n",
    "|NoTraffic|\tstring|\tWheather the airport has less than one flight per day in the last 7 days (TRUEor FALSE)|\n",
    "|Closed|\tstring|\tIf the airport has a NOTAM which is Q-code FALC (TRUE or FALSE), which means the airport is closed |\n",
    "|traffic|\tstring|\tTraffic data of the reference week, previous week and current week (json stringified format)|\n",
    "|notams|\tstring|\tNOTAMS containing COVID or CORONAVIRUS key words for the airport (json stringified format)|\n",
    "|messages| string| NOTAMS message as a string|\n",
    "|Qcode| string| Qcode of the NOTAM|\n",
    "|createdDate| datetime| NOTAM created date|\n",
    "\n",
    "Qcode reference: https://www.notams.faa.gov/common/qcode/qcode.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_airport_codes = pd.read_csv(\"/project_data/data_asset/all_airports_covid_notams_20200525.csv\")['airportCode'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############\n",
    "#Stop words#\n",
    "############\n",
    "\n",
    "nlp_ = spacy.load('en_core_web_md')\n",
    "\n",
    "# Adding stop words\n",
    "new_stop_words = [\"create\",\"source\",\"euecyiyn\",'etczyoyx','tel']\n",
    "\n",
    "# Add airport codes to stop words\n",
    "new_stop_words.extend([ac.lower() for ac in list(all_airport_codes)])\n",
    "\n",
    "\n",
    "for new_word in new_stop_words:\n",
    "    nlp_.vocab[new_word].is_stop = True\n",
    "\n",
    "# Add language detector to pipeline\n",
    "nlp_.add_pipe(LanguageDetector(), name='language_detector', last=True)\n",
    "    \n",
    "\n",
    "#https://proairpilot.com/faa-notam.html\n",
    "#https://www.icao.int/NACC/Documents/Meetings/2014/ECARAIM/REF03-ICAOCodes.pdf\n",
    "mapping = {\"acc\": \"area control\", \"acft\": \"aircraft\", \"ad\": \"aerodrome\", \"aic\": \"aeronautical information circular\",\n",
    "           \"aip\": \"aeronautical information publication\", \"ais\": \"aeronautical information services\",\n",
    "           \"alt\": \"altitude\", \"altn\": \"alternate\", \"ap\": \"airport\", \"aro\": \"air traffic services reporting office\",\n",
    "           \"arr\": \"arrival\", \"atc\": \"air traffic control\", \"ats\": \"air traffic services\", \"attn\": \"attention\",\n",
    "           \"auth\": \"authorized\", \"avbl\": \"available\", \"bfr\": \"before\", \"cat\": \"category\", \"chg\": \"change\",\"civ\":\"civil\",\n",
    "           \"clsd\": \"closed\", \"cov\": \"cover\", \"cta\": \"control area\", \"ctc\": \"contact\", \"ctr\": \"control zone\",\n",
    "           \"dem.\": \"democratic\", \"dep\": \"depart\", \"emerg\": \"emergency\", \"enr\": \"en route\", \"exc\": \"except\",\n",
    "           \"fed.\": \"federation\", \"fir\": \"flight information region\", \"fis\": \"flight information service\",\n",
    "           \"flt\": \"flight\", \"flts\": \"flights\", \"flw\": \"follows\", \"fm\": \"from\", \"fpl\": \"filed flight plan\",\n",
    "           \"fri\": \"friday\", \"gen\": \"general\", \"hr\": \"hour\", \"intl\": \"international\", \"isl.\": \"islands\",\n",
    "           \"ldg\": \"landing\", \"mil\": \"military\", \"mon\": \"monday\", \"op\": \"operation\",\"ops\": \"operations\", \n",
    "           \"opr\": \"operating\",\"pax\": \"passenger\",\n",
    "           \"ppr\": \"prior permission required\", \"ref\": \"refernce to\", \"rep.\": \"republic\", \"req\": \"request\",\n",
    "           \"rffs\": \"rescue and fire fighting services\", \"rmk\": \"remark\", \"rte\": \"route\", \"rwy\": \"runway\",\n",
    "           \"sat\": \"saturday\", \"ser\": \"service\", \"svc\": \"service message\", \"taf\": \"terminal aerodrome forecast\",\n",
    "           \"tfc\": \"traffic\", \"thu\": \"thursday\", \"tma\": \"terminal control area\", \"tue\": \"tuesday\",\n",
    "           \"twr\": \"aerodrome control tower\", \"vfr\": \"visual flight rules\"}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Cleaning the NOTAMs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_message(message):\n",
    "    #############################\n",
    "    ## mapping, lower, language #\n",
    "    #############################\n",
    "    #Make everything to lower case\n",
    "    message = row['message'].lower().strip()\n",
    "    message = re.sub(r'(http|https|www)\\S+', '', message)\n",
    "    \n",
    "    # Mapping short terms to actual word\n",
    "    for s_, w in mapping.items():\n",
    "        message = re.sub(r'\\b{}\\b'.format(s_),w,message)\n",
    "    \n",
    "    # If a country has NOTAMs in two languages, the english text starts with the phrase \"english text/english version\"\n",
    "    if \"english text\" in message:\n",
    "        #print(\"TEXT\")\n",
    "        e_t = []\n",
    "        for m_ in message.split(\"english text\"):\n",
    "            for sent_ in sent_tokenize(m_):\n",
    "                if detect(sent_) == \"en\":\n",
    "                    e_t.append(sent_)\n",
    "        message = \"\".join(e_t)\n",
    "\n",
    "    elif \"english version\" in message:\n",
    "        #print(\"VERSION\")\n",
    "        e_t = []\n",
    "        for m_ in message.split(\"english version\"):\n",
    "            for sent_ in sent_tokenize(m_):\n",
    "                if detect(sent_) == \"en\":\n",
    "                    e_t.append(sent_)\n",
    "        \n",
    "        message = \"\".join(e_t)\n",
    "        #message = message.split(\"english version\")[1]\n",
    "    elif detect(message) != \"en\":\n",
    "        message = \"\"\n",
    "\n",
    "    #Start of string other than character or digit\n",
    "    message = re.sub(r'[^ 0-9a-z]', ' ', message)\n",
    "    message = message.translate(message.maketrans('', '', string.punctuation)) #extra punctuations removal\n",
    "\n",
    "    # Remove unnecessary white space\n",
    "    message = \" \".join(message.split())\n",
    "    \n",
    "    return message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Generating tokens**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tokens(message):\n",
    "    sent_ = nlp_(message)\n",
    "    # Cleaning text\n",
    "    tokens = []\n",
    "    for token in sent_:\n",
    "        # Remove punctuation and numbers\n",
    "        # Get only date digits!\n",
    "        if token.is_alpha:\n",
    "            # Lemma\n",
    "            lemma_text = token.lemma_\n",
    "            #Remove stop words\n",
    "            if not nlp_.vocab[lemma_text].is_stop:\n",
    "                if len(lemma_text) > 2:\n",
    "                    tokens.append(lemma_text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Airspace COVID-19 NOTAMS - one file per week**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "files_dt = dict()\n",
    "files_dt['download_date'] = []\n",
    "files_dt['download_week'] = []\n",
    "files_dt['file_location'] = []\n",
    "for file_ in os.listdir(\"/project_data/data_asset/\"):\n",
    "    if file_.startswith(\"all_air\"):\n",
    "        #print(file_)\n",
    "        files_dt['download_date'].append(datetime.datetime.strptime(file_.split(\"_\")[-1].split(\".csv\")[0],\"%Y%m%d\"))\n",
    "        files_dt['download_week'].append(datetime.datetime.strptime(file_.split(\"_\")[-1].split(\".csv\")[0],\"%Y%m%d\").isocalendar()[1])\n",
    "        files_dt['file_location'].append(os.path.join(\"/project_data/data_asset/\",file_))\n",
    "\n",
    "# The file with the latest date for the selected week is considered\n",
    "files_df = pd.DataFrame(files_dt)\n",
    "weekly_files_df = files_df[files_df['download_date'] == files_df.groupby(\"download_week\",sort=\"as\")[\"download_date\"].transform(\"max\")]\n",
    "\n",
    "# Iterate one file per week!\n",
    "for idx, r in weekly_files_df.iterrows():\n",
    "    if \"all_airspaces\" in r['file_location']:\n",
    "        print(r['file_location'])\n",
    "        \n",
    "        asp_week_df = pd.read_csv(r['file_location'])\n",
    "        \n",
    "        # Expand the jsons in the dataframe\n",
    "        asp_df = decode_airspace_message(asp_week_df)\n",
    "        \n",
    "        # Preprocess the data\n",
    "        asp_df['tokens'] = None\n",
    "        asp_df['cleaned_message'] = None\n",
    "\n",
    "        for idx,row in asp_df.iterrows():\n",
    "            if row['message'] is not None:\n",
    "                message = row['message']\n",
    "\n",
    "                message_ = clean_message(message)\n",
    "                tokens = generate_tokens(message_)\n",
    "\n",
    "                asp_df.at[idx,\"cleaned_message\"] = message_\n",
    "                asp_df.at[idx,\"tokens\"] = tokens\n",
    "        \n",
    "        # Remove na rows\n",
    "        valid_asp_df = asp_df.dropna(subset=['message'])\n",
    "        valid_asp_df = valid_asp_df[valid_asp_df.cleaned_message != '']\n",
    "        \n",
    "        # Add the date components as columns\n",
    "        valid_asp_df['active_week']=r['download_week']\n",
    "        valid_asp_df['download_date'] = r['download_date']\n",
    "        print(len(valid_asp_df))\n",
    "        valid_asp_df.to_csv(\"/project_data/data_asset/notams/valid_airspace_notams_\"+str(r['download_week'])+\".csv\", index=False,quoting=csv.QUOTE_NONNUMERIC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Unresolved abbreviations**\n",
    "\n",
    "de, del,atr,sar (this refes to special administrative zone as well as search and rescue),\n",
    "dgac (some authority center),caa(some authority center),enac(some authority center),hum,act (multiple meanings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Qcode**\n",
    "\n",
    "Qcode is a brevity code that informs the type of message being sent. For our analysis we can ignore some Qcodes such as aerodrome service hours as we are mainly looking for quarantine duration as well as country restrictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "|Qcode| Meaning|\n",
    "|----|-----|\n",
    "|FAXX | Aerodrome - other|\n",
    "|FAAH | Aerodrome - HOURS OF SERVICE ARE|\n",
    "|FALT | Aerodrome - LIMITED TO|\n",
    "|FFCG | FIRE FIGHTING AND RESCUE - DOWNGRADED TO|\n",
    "|XXXX | Other - Other|\n",
    "|FALC | Aerodrome - CLOSED|\n",
    "|ACXX | CLASS B, C, D OR E SURFACE AREA (ICAO-CONTROL ZONE) - OTHER|\n",
    "|FAAP | Aerodrome - PRIOR PERMISSION REQUIRED|\n",
    "|SPAH | APPROACH CONTROL - HOURS OF SERVICE ARE|\n",
    "|AFXX |FLIGHT INFORMATION REGION (FIR) - OTHER|\n",
    "|OEXX | AIRCRAFT ENTRY REQUIREMENTS - OTHER| \n",
    "|OECA |AIRCRAFT ENTRY REQUIREMENTS - |\n",
    "|OAXX |AERONAUTICAL INFORMATION SERVICE - OTHER|\n",
    "|SEAH |FLIGHT INFORMATION SERVICE -HOURS OF SERVICE ARE |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Qcode to be excluded: FAAH, FFCG, SPAH other codes that end with AH\n",
    "    \n",
    "Qcode of interest: FAXX, FALC, FALT, OEXX, OECA\n",
    "    \n",
    "Qcode not sure: ACXX, FAAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Author**\n",
    "\n",
    "* Shri Nishanth Rajendran - AI Development Specialist, R² Data Labs, Rolls Royce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
