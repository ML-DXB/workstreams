{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling of Airport NOTAMs\n",
    "\n",
    "\n",
    "In this notebook, we read the preprocessed NOTAMs data and identify differnt topics present in the messages. This exercise is based on the topic modeling analysis carried out on News data: https://github.com/emergent-analytics/workstreams/blob/master/ws2/news-analysis/%20ws2_2_topic_modelling.ipynb \n",
    "\n",
    "As the Qcodes present in the data also correspond to the nature of NOTAMs, we use topic modeling to ascertain our assumption\n",
    "\n",
    "**Input**\n",
    "\n",
    "  To generate the input dataset, refer this notebook: ws2_snr_NOTAMs_1_data_preparation\n",
    "  \n",
    "  Preprocessed airport dataset\n",
    "  \n",
    "  - valid_airport_notams_xx.csv\n",
    "\n",
    "**Output**\n",
    "\n",
    "Dataset with identified topics\n",
    "  \n",
    "  - valid_airport_notams_with_topics_xx.csv\n",
    "  \n",
    "Visualisation of topic modeling results\n",
    "  \n",
    "  \n",
    "  - covid_airport_notams_lda.html\n",
    "\n",
    "where 'xx' corresponds to the date\n",
    "\n",
    "\n",
    "  \n",
    "The following steps are carried out:\n",
    "\n",
    "    1. Import the preprocessed data\n",
    "\n",
    "    2. Filter out NOTAMs related to service hours \n",
    "\n",
    "    3. Train the LDA model and compute the coherence metric\n",
    "    \n",
    "    4. Visualize the topics\n",
    "    \n",
    "    5. LDA as feature\n",
    "    \n",
    "    6. Map manual labels to topics\n",
    "    \n",
    "    7. Analyse the results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import spacy\n",
    "except:\n",
    "    !pip install spacy\n",
    "try:\n",
    "    import spacy_langdetect\n",
    "except:\n",
    "    !pip install spacy-langdetect\n",
    "try:\n",
    "    import flair\n",
    "except:\n",
    "    !pip install flair\n",
    "try:\n",
    "    import geonamescache\n",
    "except:\n",
    "    !pip install geonamescache\n",
    "try:\n",
    "    import spacy_fastlang\n",
    "except:    \n",
    "    !pip install spacy_fastlang\n",
    "    #!pip install sense2vec==1.0.0a1\n",
    "try:\n",
    "    import gensim\n",
    "except:\n",
    "    !pip install gensim\n",
    "try:\n",
    "    import wordcloud\n",
    "except:\n",
    "    !pip install wordcloud\n",
    "try:\n",
    "    import nltk\n",
    "except:\n",
    "    !pip install nltk\n",
    "\n",
    "try:\n",
    "    import pyLDAvis\n",
    "except:\n",
    "    !pip install pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "import itertools\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import string\n",
    "\n",
    "from spacy_langdetect import LanguageDetector\n",
    "import plac\n",
    "from spacy.lang.en import English\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.tokens import Doc, Span, Token\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('tagsets')\n",
    "nltk.download('words')\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "from spacy import displacy\n",
    "import seaborn as sbs\n",
    "import geonamescache\n",
    "import ast\n",
    "\n",
    "#!pip install gensim\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "#!pip install pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Import the preprocessed data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apt_df_ = pd.read_csv(\"/project_data/data_asset/ws2/notams/valid_airport_notams_20200703.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apt_df_.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Filter out NOTAMs related to service hours**\n",
    "\n",
    "Filtering out NOTAMs related to service hours and fire fighting rescue information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apt_lda_df = apt_df_[~((apt_df_.Qcode.str.endswith(\"AH\")) | (apt_df_.Qcode == \"FFCG\"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apt_lda_df.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Train the LDA model and compute the coherence metric**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "for text in apt_lda_df['tokens']:\n",
    "    words.append(ast.literal_eval(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the term dictionary of courpus\n",
    "dictionary = corpora.Dictionary(words)\n",
    "\n",
    "# filter the least and most frequent words: filters if less than no_below, more than no_above\n",
    "dictionary.filter_extremes(no_below=10, no_above=0.9) \n",
    "dictionary.compactify()\n",
    "\n",
    "# convert list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
    "doc_term_matrix = [dictionary.doc2bow(word) for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train LDA, computing the coherence score for a range of topics\n",
    "coherence_scores = []\n",
    "\n",
    "for num_topics in range(2, 12, 1):\n",
    "    \n",
    "    print(f\"Number of topics: \", num_topics)\n",
    "    \n",
    "    # create the object for LDA model using gensim library\n",
    "    Lda = gensim.models.ldamulticore.LdaMulticore\n",
    "\n",
    "    # run and train LDA model on the document term matrix.\n",
    "    ldamodel = Lda(doc_term_matrix, \n",
    "                   num_topics=num_topics, \n",
    "                   id2word = dictionary, \n",
    "                   passes=20, \n",
    "                   chunksize = 2000, \n",
    "                   random_state=42,\n",
    "                   workers=6)\n",
    "    \n",
    "    # compute the coherence score\n",
    "    coherence_model = CoherenceModel(model=ldamodel, \n",
    "                                     texts=words, \n",
    "                                     dictionary=dictionary, \n",
    "                                     coherence='c_v')\n",
    "\n",
    "    coherence_lda = coherence_model.get_coherence()\n",
    "    \n",
    "    coherence_scores.append((num_topics, coherence_lda))\n",
    "\n",
    "coherence_scores = [*zip(*coherence_scores)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the coherence score for topics\n",
    "plt.plot(coherence_scores[0], coherence_scores[1], marker='o')\n",
    "plt.title('Coherence Score for Topics')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the above plot shows that there are 4 main topics in airport NOTAMs we assign the number of topics to 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the number of topics where coherence score is the highest\n",
    "num_topics = 4\n",
    "\n",
    "# run and train LDA model on the document term matrix.\n",
    "Lda = gensim.models.ldamulticore.LdaMulticore\n",
    "\n",
    "ldamodel = Lda(doc_term_matrix, \n",
    "               num_topics=num_topics, \n",
    "               id2word=dictionary, \n",
    "               passes=20, \n",
    "               chunksize=10000, \n",
    "               random_state=42,\n",
    "               workers=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view the topics with their most important words and their proportions\n",
    "ldamodel.print_topics(num_topics=num_topics, num_words=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Visualize the topics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the intractive LDA plot\n",
    "lda_display = pyLDAvis.gensim.prepare(ldamodel, \n",
    "                                      doc_term_matrix, \n",
    "                                      dictionary, \n",
    "                                      sort_topics=False)\n",
    "pyLDAvis.display(lda_display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the plot in html format\n",
    "pyLDAvis.save_html(lda_display, f\"/project_data/data_asset/ws2/notams/covid_airport_notams_lda.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. LDA as feature**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user inputs\n",
    "corpus = doc_term_matrix\n",
    "texts = apt_lda_df\n",
    "df = apt_lda_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(apt_lda_df),len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get dominant topic, percentage of contribution, and keywords for each document\n",
    "def format_topics_sentences(ldamodel, corpus):\n",
    "\n",
    "    results = []\n",
    "    \n",
    "    # get main topic in each document\n",
    "    for row in ldamodel[corpus]:\n",
    "        \n",
    "        if len(row) == 0:\n",
    "            continue\n",
    "            \n",
    "        row = list(sorted(row, key=lambda elem: elem[1], reverse=True))\n",
    "        \n",
    "        # get the dominant topic, percentage of contribution and keywords for each document\n",
    "        topic_num, prop_topic = row[0]        \n",
    "        wp = ldamodel.show_topic(topic_num)\n",
    "        topic_keywords = \", \".join([word for word, prop in wp])\n",
    "        results.append((topic_num, round(prop_topic, 4), [topic_keywords]))\n",
    "    \n",
    "    df = pd.DataFrame.from_records(results, columns=['dominant_topic', 'weight', 'keywords'])\n",
    "    \n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topics = format_topics_sentences(ldamodel, corpus)\n",
    "df_topics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate with the main dataset\n",
    "apt_lda_df_ = pd.concat([apt_lda_df, df_topics.reindex(apt_lda_df.index)], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. Map manual labels to topics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the topic labels for all the topics identified.\n",
    " \n",
    "topics_dict = [[0, 'label_1'],\n",
    "               [1, 'label_2'], \n",
    "               [2, 'label_3'], \n",
    "               [3, 'label_4']]\n",
    "\n",
    "labels = pd.DataFrame(topics_dict, columns =['topic_num', 'topic_label'])\n",
    "\n",
    "# merge with the main dataset\n",
    "apt_lda_df_ = pd.merge(apt_lda_df_, labels, how='left', left_on = 'dominant_topic', right_on='topic_num')\n",
    "apt_lda_df_.drop(\"topic_num\", axis=1, inplace=True)\n",
    "apt_lda_df_.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apt_lda_df_.to_csv(\"/project_data/data_asset/ws2/notams/valid_airport_notams_with_topics_20200703.csv\", index=False, quoting=csv.QUOTE_NONNUMERIC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7. Analyse results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for l in [\"label_1\",\"label_2\",\"label_3\",\"label_4\"]:\n",
    "    d_ = apt_lda_df_[apt_lda_df_.topic_label==l]\n",
    "    print(l)\n",
    "    print(len(d_))\n",
    "    print(d_.Qcode.value_counts()[:5])\n",
    "    #print(d_.countryName.unique())\n",
    "    print(d_.keywords.values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for label_ in ['label_1','label_2','label_3','label_4']:\n",
    "    all_t = []\n",
    "    for t in apt_lda_df_[apt_lda_df_.topic_label==label_]['tokens']:\n",
    "        for t_ in ast.literal_eval(t):\n",
    "            all_t.append(t_)\n",
    "\n",
    "    wc = WordCloud(background_color=\"white\", max_words=200, random_state=1,collocations=False).generate(' '.join(all_t))# to recolour the image\n",
    "    plt.figure(figsize=(15,5)) #, width=1400, height=800,\n",
    "    plt.title(\"word cloud for - {}\".format(label_))\n",
    "    plt.grid(b=None)\n",
    "    plt.imshow(wc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:**\n",
    "\n",
    "Based on pyLDAvis and wordcloud of the different topics the following insights have been noted:\n",
    "\n",
    "1. Label 3 contains information related to cargo\n",
    "\n",
    "2. Labels 1 and 3 contain all the information related to quarantine\n",
    "\n",
    "3. Label 2 contains information related to visual flight rules\n",
    "\n",
    "4. Label 4 contains information more related to control tower as the terms in this label include tower, control, frequency, mhz (unit of frequency)\n",
    "\n",
    "\n",
    "In terms of commercial passenger flights, topics with labels 1 and 3 are to be considered for further analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Author**\n",
    "\n",
    "* Shri Nishanth Rajendran - AI Development Specialist, R² Data Labs, Rolls Royce\n",
    "\n",
    "The topic modelling work is based on the analysis done below:\n",
    "https://github.com/emergent-analytics/workstreams/blob/master/ws2/news-analysis/%20ws2_2_topic_modelling.ipynb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
