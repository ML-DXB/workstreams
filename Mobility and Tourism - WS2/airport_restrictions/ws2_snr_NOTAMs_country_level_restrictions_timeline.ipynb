{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Timeline of foreign country restriction for each country\n",
    "\n",
    "To create a timeline (historical record of country restrictions) we use run the information extraction code on Airspace NOTAMs files since last week of May 2020. We use one file per week.\n",
    "\n",
    "## Goal\n",
    "\n",
    "* Extract country specific restriction from NOTAMs. Identify passenger/flights from which countries are allowed to land in a country\n",
    "\n",
    "    \n",
    "    - The following is a NOTAM from one of the airspaces in Hungary:\n",
    "    \n",
    "            arrival entry at hungarian airports is allowed only for hungarian citizens and citizens of andorra, austria, belgium, cyprus, czechia, denmark, estonia, finland, france, greece, netherlands, croatia, ireland, iceland, polen, latvia, lichtenstein, lithuania, luxembourg, malta, monaco, germany, italy, san marino, spain, switzerland, slovakia, slovenia and vatican.\n",
    "            \n",
    "            \n",
    "### Tasks and Challenges\n",
    "\n",
    "* Identify mention of countries or group of countries, etc in the message and tag it!\n",
    "\n",
    "    - Country mentioned using multiple abbreviations, aliases (example: U.S.A, U.S., United States)\n",
    "    \n",
    "    - Mention of group of countries like European Union, Schengen, Latin America, Carribean, etc\n",
    "\n",
    "\n",
    "* Extract the meaning of the sentence \n",
    "\n",
    "    - Identify the type - entry closed to the mentioned countries or open to the mentioned countries or some exceptions apply\n",
    "    \n",
    "    - In some cases the airport is closed to all flights. This should also be captured as closed to all countries\n",
    "    \n",
    "    - In some cases the sentence contains information about countries allowed and that are not allowed and it has to be discerned!\n",
    "    \n",
    "    \n",
    "### Input\n",
    "\n",
    "- Output files of the notebook: ws2_snr_NOTAMs_1_data_preparation_mulitple files\n",
    "\n",
    "- all_countries_with_aliases.csv\n",
    "\n",
    "### Output\n",
    "\n",
    "- country_closure_timeline.csv\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import spacy\n",
    "except:\n",
    "    !pip install spacy\n",
    "try:\n",
    "    import spacy_langdetect\n",
    "except:\n",
    "    !pip install spacy-langdetect\n",
    "try:\n",
    "    import flair\n",
    "except:\n",
    "    !pip install flair\n",
    "try:\n",
    "    import geonamescache\n",
    "except:\n",
    "    !pip install geonamescache\n",
    "try:\n",
    "    import spacy_fastlang\n",
    "except:    \n",
    "    !pip install spacy_fastlang\n",
    "    #!pip install sense2vec==1.0.0a1\n",
    "try:\n",
    "    import gensim\n",
    "except:\n",
    "    !pip install gensim\n",
    "try:\n",
    "    import wordcloud\n",
    "except:\n",
    "    !pip install wordcloud\n",
    "try:\n",
    "    import nltk\n",
    "except:\n",
    "    !pip install nltk\n",
    "\n",
    "try:\n",
    "    import visualise_spacy_tree\n",
    "except:\n",
    "    !pip install visualise-spacy-tree\n",
    "\n",
    "try:\n",
    "    import textacy\n",
    "except:\n",
    "    !pip install textacy\n",
    "\n",
    "!python -m spacy download en_core_web_sm\n",
    "!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import requests\n",
    "import plac\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.tokens import Doc, Span, Token\n",
    "from spacy.pipeline import EntityRuler\n",
    "from spacy.pipeline import merge_entities\n",
    "import csv\n",
    "import re\n",
    "import ast\n",
    "from langdetect import detect\n",
    "import numpy as np\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "import visualise_spacy_tree\n",
    "import textacy\n",
    "\n",
    "from IPython.display import Image,display\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Identify mention of countries or group of countries, etc in the message and tag it!\n",
    "\n",
    "- Identification of country names in NOTAMS:\n",
    "\n",
    "\n",
    "    - Collect all the aliases of country names\n",
    "    - Create a pattern to tag it with the country code as the id\n",
    "    - As some countries have more than single word, merge country names to single entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_countries = pd.read_csv(\"/project_data/data_asset/all_countries_with_aliases.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_countries.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_countries_aliases = dict()\n",
    "for idx, r in all_countries[~all_countries.Aliases.isna()].iterrows():\n",
    "    a = dict()\n",
    "    a['alpha_2_code'] = r['Alpha-2 code']\n",
    "    a['alpha_3_code'] = r['Alpha-3 code']\n",
    "    a['name'] = r['Name']\n",
    "    all_countries_aliases.update({r['Name'].lower():a})\n",
    "    \n",
    "    cnames = r['Aliases'].split(' # ')\n",
    "    if len(cnames) > 0:\n",
    "        for c in cnames:\n",
    "            if len(c.strip())>3:\n",
    "                a = dict()\n",
    "                a['alpha_2_code'] = r['Alpha-2 code']\n",
    "                a['alpha_3_code'] = r['Alpha-3 code']\n",
    "                a['name'] = r['Name']\n",
    "                all_countries_aliases.update({c.strip().lower():a})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_patterns = list()\n",
    "for k in all_countries_aliases.keys():\n",
    "    c_patterns = k.split()\n",
    "\n",
    "    \n",
    "    c_patterns_list = []\n",
    "    for c in c_patterns:\n",
    "        d = dict()\n",
    "        d['LOWER'] = c\n",
    "        c_patterns_list.append(d)\n",
    "    \n",
    "    \n",
    "    pattern_dict = dict()\n",
    "    pattern_dict[\"label\"] = \"GPE\"\n",
    "    pattern_dict[\"pattern\"] = c_patterns_list\n",
    "    pattern_dict[\"id\"] = all_countries_aliases[k]['alpha_3_code']\n",
    "    \n",
    "    all_patterns.append(pattern_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eu = [\"Belgium\",\n",
    "    \"Bulgaria\",\n",
    "    \"Denmark\",\n",
    "    \"Germany\",\n",
    "    \"Estonia\",\n",
    "    \"Finland\",\n",
    "    \"France\",\n",
    "    \"Greece\",\n",
    "    \"Ireland\",\n",
    "    \"Italy\",\n",
    "    \"Croatia\",\n",
    "    \"Latvia\",\n",
    "    \"Lithuania\",\n",
    "    \"Luxembourg\",\n",
    "    \"Malta\",\n",
    "    \"Netherlands\",\n",
    "    \"Austria\",\n",
    "    \"Poland\",\n",
    "    \"Portugal\",\n",
    "    \"Romania\",\n",
    "    \"Sweden\",\n",
    "    \"Slovakia\",\n",
    "    \"Slovenia\",\n",
    "    \"Spain\",\n",
    "    \"Czech Republic\",\n",
    "    \"Hungary\",\n",
    "    \"United Kingdom\",\n",
    "    \"Cyprus\"]\n",
    "\n",
    "eea_c = eu+[\n",
    "    \"Iceland\",\n",
    "    \"Liechtenstein\",\n",
    "    \"Norway\",\"Switzerland\"\n",
    "]\n",
    "\n",
    "schengen = [\"Austria\", \"Belgium\", \"Czech Republic\", \"Denmark\", \"Estonia\", \"Finland\", \"France\", \"Germany\", \"Greece\", \"Hungary\", \"Iceland\", \"Italy\", \"Latvia\",\n",
    "            \"Liechtenstein\", \"Lithuania\", \"Luxembourg\", \"Malta\", \"Netherlands\", \"Norway\", \"Poland\", \"Portugal\", \"Slovakia\", \"Slovenia\", \"Spain\", \"Sweden\", \"Switzerland\"]\n",
    "\n",
    "scandinavian = [\"Denmark\",\"Norway\",\"Sweden\"]\n",
    "\n",
    "eum = []\n",
    "for c in eu:\n",
    "    #print(all_countries[all_countries['Name'].str.contains(c)]['Alpha-3 code'].values[0])\n",
    "    eum.append(all_countries[all_countries['Name'].str.contains(c)]['Alpha-3 code'].values[0])\n",
    "\n",
    "eea = []\n",
    "for c in eea_c:\n",
    "    #print(all_countries[all_countries['Name'].str.contains(c)]['Alpha-3 code'].values[0])\n",
    "    eea.append(all_countries[all_countries['Name'].str.contains(c)]['Alpha-3 code'].values[0])\n",
    "\n",
    "sch = []\n",
    "for c in schengen:\n",
    "    #print(all_countries[all_countries['Name'].str.contains(c)]['Alpha-3 code'].values[0])\n",
    "    sch.append(all_countries[all_countries['Name'].str.contains(c)]['Alpha-3 code'].values[0])\n",
    "\n",
    "svc = []\n",
    "for c in scandinavian:\n",
    "    #print(all_countries[all_countries['Name'].str.contains(c)]['Alpha-3 code'].values[0])\n",
    "    svc.append(all_countries[all_countries['Name'].str.contains(c)]['Alpha-3 code'].values[0])\n",
    "    \n",
    "neum = list(set(all_countries['Alpha-3 code'].unique()).difference(set(eum)))\n",
    "nsch = list(set(all_countries['Alpha-3 code'].unique()).difference(set(sch)))\n",
    "neu = list(set(all_countries['Alpha-3 code'].unique()).difference(set(eea)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating patterns to identify group of countries such as European Union, etc and add it to entity ruler so that mention of such words would be captured as GPE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_patterns.extend([{'label': 'GPE',\n",
    "  'pattern': [{'LOWER': 'republic'}, {'LOWER': 'of'}, {'LOWER': 'costarica'}],\n",
    "  'id': 'CRI'},\n",
    "  {'label': 'GPE',\n",
    "  'pattern': [{'LOWER': 'saint'}, {'LOWER': 'christopher'}, {'LOWER': 'and'}, {'LOWER': 'nevis'}],\n",
    "  'id': 'KNA'}, \n",
    "{'label': 'GPE',\n",
    "  'pattern': [{'LOWER': 'vatican'}, {'LOWER': 'city'}],\n",
    "  'id': 'VAT'},\n",
    "{'label': 'GPE',\n",
    "  'pattern': [{'LOWER': 'vatican'}],\n",
    "  'id': 'VAT'},\n",
    "{'label': 'GPE',\n",
    "  'pattern': [{'LOWER': 'member'}, {'LOWER': 'states'}, {'LOWER': 'of'},{'LOWER': 'the'}, {'LOWER': 'european'}, {'LOWER': 'union'}],\n",
    "  'id': 'EUM'},\n",
    "{'label': 'GPE',\n",
    "  'pattern': [{'LOWER': 'eu'}, {'LOWER': 'member'}, {'LOWER': 'state'}],\n",
    "  'id': 'EUM'},\n",
    "{'label': 'GPE',\n",
    "  'pattern': [{'LOWER': 'european'}, {'LOWER': 'union'}, {'LOWER': 'member'}, {'LOWER': 'states'}],\n",
    "  'id': 'EUM'},\n",
    "{'label': 'GPE',\n",
    "  'pattern': [{'LOWER': 'european'}, {'LOWER': 'union'}],\n",
    "  'id': 'EUM'},\n",
    "{'label': 'GPE',\n",
    "  'pattern': [{'LOWER': 'eu'}, {'LOWER': 'member'}, {'LOWER': 'states'}],\n",
    "  'id': 'EUM'},\n",
    "{'label': 'GPE',\n",
    "  'pattern': [{'LOWER': 'eu'}],\n",
    "  'id': 'EUM'},\n",
    "{'label': 'GPE',\n",
    "  'pattern': [{'LOWER': 'schengen'}],\n",
    "  'id': 'SCH'},\n",
    "{'label': 'GPE',\n",
    "  'pattern': [{'LOWER': 'south-korea'}],\n",
    "  'id': 'KOR'},\n",
    "{'label': 'GPE',\n",
    "  'pattern': [{'LOWER': 'uk'}],\n",
    "  'id': 'GBR'},\n",
    " {'label': 'GPE',\n",
    "  'pattern': [{'LOWER': 'great'}, {'LOWER': 'britain'}],\n",
    "  'id': 'GBR'},\n",
    " {'label': 'GPE',\n",
    "  'pattern': [{'LOWER': 'great'}, {'LOWER': 'britain'}, {'LOWER': 'and'}, {'LOWER': 'northern'}, {'LOWER': 'ireland'}],\n",
    "  'id': 'GBR'},\n",
    "{'label': 'GPE',\n",
    "  'pattern': [{'LOWER': 'tunesia'}],\n",
    "  'id': 'TUN'},\n",
    "{'label': 'GPE',\n",
    "  'pattern': [{'LOWER': 'scandinavian'}, {'LOWER': 'countries'}],\n",
    "  'id': 'SVC'},\n",
    "{'label': 'GPE',\n",
    "  'pattern': [{'LOWER': 'hongkong'}],\n",
    "  'id': 'HKG'},\n",
    "{'label': 'GPE',\n",
    "  'pattern': [{'LOWER': 'usa'}],\n",
    "  'id': 'USA'},\n",
    "{'label': 'GPE',\n",
    "  'pattern': [{'LOWER': 'uae'}],\n",
    "  'id': 'ARE'},\n",
    "{'label': 'GPE',\n",
    "  'pattern': [{'LOWER': 'u.s.a'}],\n",
    "  'id': 'USA'},\n",
    "{'label': 'GPE',\n",
    "  'pattern': [{'LOWER': 'u.s.'}],\n",
    "  'id': 'USA'},\n",
    "{'label': 'GPE',\n",
    "  'pattern': [{'LOWER': 'prc'}],\n",
    "  'id': 'CHN'},\n",
    "{'label': 'GPE',\n",
    "  'pattern': [{'LOWER': 'third'},{'LOWER': 'country'}],\n",
    "  'id': 'NEU'},\n",
    "{'label': 'GPE',\n",
    "  'pattern': [{'LOWER': 'third'},{'LOWER': 'countries'}],\n",
    "  'id': 'NEU'},\n",
    "{'label': 'GPE',\n",
    "  'pattern': [{'LOWER': 'third-country'}],\n",
    "  'id': 'NEU'},\n",
    "{'label': 'GPE',\n",
    "  'pattern': [{'LOWER': 'non-eu'}],\n",
    "  'id': 'NEUM'},\n",
    "{'label': 'GPE',\n",
    "  'pattern': [{'LOWER': 'non'},{'LOWER': 'eu'}],\n",
    "  'id': 'NEUM'},\n",
    "{'label': 'GPE',\n",
    "  'pattern': [{'LOWER': 'non-european'},{'LOWER': 'union'},{'LOWER': 'member'},{'LOWER': 'states'}],\n",
    "  'id': 'NEUM'},\n",
    "{'label': 'GPE',\n",
    "  'pattern': [{'LOWER': 'non-schengen'}],\n",
    "  'id': 'NSCH'},\n",
    "{'label': 'GPE',\n",
    "  'pattern': [{'LOWER': 'non'},{'LOWER': 'schengen'}],\n",
    "  'id': 'NSCH'}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_patterns[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EUM - European Member State\n",
    "# SCH - Schengen\n",
    "# SVC - Scandinavian\n",
    "# NEU - third country\n",
    "# NEUM - Non- EU Member State\n",
    "# NSCH - Non schengen\n",
    "european_regions =  ['EUM','SCH','SVC','NEU','NEUM','NSCH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "european_regions_dt = {'EUM':eum,'SCH':sch,'SVC':svc,'NEU':neu,'NEUM':neum,'NSCH':nsch}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nlp = English()\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "ruler = EntityRuler(nlp,overwrite_ents=True)\n",
    "#patterns = [{\"label\": \"ORG\", \"pattern\": \"Apple\", \"id\": \"apple\"},\n",
    "#            {\"label\": \"GPE\", \"pattern\": [{\"LOWER\": \"san\"}, {\"LOWER\": \"francisco\"}], \"id\": \"san-francisco\"},\n",
    "#            {\"label\": \"GPE\", \"pattern\": [{\"LOWER\": \"san\"}, {\"LOWER\": \"fran\"}], \"id\": \"san-francisco\"}]\n",
    "ruler.add_patterns(all_patterns)\n",
    "nlp.add_pipe(ruler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Merge entities of GPE and ORG into single tokens**\n",
    "\n",
    "Merging GPE entities help in simplifying the sentence structure mapping \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.add_pipe(merge_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = nlp(\"due to the spread of coronavirus infections, the range of foreigners who are not permitted to enter japan was expanded, effective 1st jul 2020 as follows: asia india, indonesia, republic of korea, singapore, thailand, taiwan, china, islamic republic of pakistan, people's republic of bangladesh, philippines, brunei, viet nam, malaysia, republic of maldives oceania australia, new zealand north america canada, united states of america latin america and the caribbean argentine republic, antigua and barbuda, oriental republic of uruguay, ecuador, republic of el salvador, republic of guyana, republic of cuba, republic of guatemala, grenada, republic of costarica, republic of colombia, jamaica, saint vincent and the grenadines, saint christopher and nevis, chile, dominica, dominican republic, republic of nicaragua, republic of haiti, panama, commonwealth of the bahamas, barbados, brazil, republic of peru, bolivia, republic of honduras , united mexican states to be continued\")\n",
    "print([(ent.text, ent.label_, ent.ent_id_) for ent in doc1.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.displacy.render(doc1, style='ent',jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After extending the patterns for GPE entity we are able to identify most of the countries mentioned in the NOTAMs as shown above.\n",
    "\n",
    "Below is a dependency plot of the words in the NOTAMS message:\n",
    "\n",
    "- We note that the verb \"permitted\" is connected to negation \"not\" - this shows that the sentence is about closing the border"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spacy.displacy.render(doc1, style='dep',jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "img_ = visualise_spacy_tree.create_png(doc1)\n",
    "display(Image(img_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Extract the meaning of the sentence\n",
    "\n",
    "\n",
    "Identify if the countries mentioned in the document are allowed or not allowed.\n",
    "\n",
    "Since country level restrictions are mostly taken by the Government, we consider NOTAMS from airspace rather than airport as they have a wider coverage\n",
    "\n",
    "Steps carried out:\n",
    "\n",
    "1. Load the airspace dataset\n",
    "\n",
    "2. Preprocess the text to extend abbreviations, remove hyperlinks, foreign text but leave the punctuations as we do sentence tokenisation\n",
    "\n",
    "3. Identify commonly used nouns, verbs, noun chunks and special adverbs such as \"other than\",etc. \n",
    "\n",
    "4. Create rules to extract the intent of the sentence. For example using the verb/noun mentioned in the sentence identify if the sentence is about closed to countries or open to countries\n",
    "   Once the intent is determined then extract the countries mentioned in the sentence\n",
    "   \n",
    "5. Add the results to a table and then visualise it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess the text to extend abbreviations, remove hyperlinks, foreign text but leave the punctuations as we do sentence tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding stop words\n",
    "new_stop_words = [\"create\",\"source\",\"euecyiyn\",'etczyoyx','tel']\n",
    "\n",
    "# Add airport codes to stop words\n",
    "#new_stop_words.extend([ac.lower() for ac in list(apt_covid_notam_lt_df.airportCode.values)])\n",
    "\n",
    "\n",
    "for new_word in new_stop_words:\n",
    "    nlp.vocab[new_word].is_stop = True\n",
    "\n",
    "mapping = {\"acc\": \"area control\", \"acft\": \"aircraft\", \"ad\": \"aerodrome\", \"aic\": \"aeronautical information circular\",\n",
    "           \"aip\": \"aeronautical information publication\", \"ais\": \"aeronautical information services\",\n",
    "           \"alt\": \"altitude\", \"altn\": \"alternate\", \"ap\": \"airport\", \"aro\": \"air traffic services reporting office\",\n",
    "           \"arr\": \"arrival\", \"atc\": \"air traffic control\", \"ats\": \"air traffic services\", \"attn\": \"attention\",\n",
    "           \"auth\": \"authorized\", \"avbl\": \"available\", \"bfr\": \"before\", \"cat\": \"category\", \"chg\": \"change\",\"civ\":\"civil\",\n",
    "           \"clsd\": \"closed\", \"cov\": \"cover\", \"cta\": \"control area\", \"ctc\": \"contact\", \"ctr\": \"control zone\",\n",
    "           \"dem.\": \"democratic\", \"dep\": \"depart\", \"emerg\": \"emergency\", \"enr\": \"en route\", \"exc\": \"except\",\n",
    "           \"fed.\": \"federation\", \"fir\": \"flight information region\", \"fis\": \"flight information service\",\n",
    "           \"flt\": \"flight\", \"flts\": \"flights\", \"flw\": \"follows\", \"fm\": \"from\", \"fpl\": \"filed flight plan\",\n",
    "           \"fri\": \"friday\", \"gen\": \"general\", \"hr\": \"hour\", \"intl\": \"international\", \"isl.\": \"islands\",\n",
    "           \"ldg\": \"landing\", \"mil\": \"military\", \"mon\": \"monday\", \"op\": \"operation\",\"ops\": \"operations\", \n",
    "           \"opr\": \"operating\",\"pax\": \"passenger\",\n",
    "           \"ppr\": \"prior permission required\", \"ref\": \"refernce to\", \"rep.\": \"republic\", \"req\": \"request\",\n",
    "           \"rffs\": \"rescue and fire fighting services\", \"rmk\": \"remark\", \"rte\": \"route\", \"rwy\": \"runway\",\n",
    "           \"sat\": \"saturday\", \"ser\": \"service\", \"svc\": \"service message\", \"taf\": \"terminal aerodrome forecast\",\n",
    "           \"tfc\": \"traffic\", \"thu\": \"thursday\", \"tma\": \"terminal control area\", \"tue\": \"tuesday\",\n",
    "           \"twr\": \"aerodrome control tower\", \"vfr\": \"visual flight rules\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_message_keep_punct(message):\n",
    "    #############################\n",
    "    ## mapping, lower, language #\n",
    "    #############################\n",
    "    message = re.sub(r'(http|https|www)\\S+', '', message)\n",
    "    \n",
    "    # Mapping short terms to actual word\n",
    "    for s_, w in mapping.items():\n",
    "        message = re.sub(r'\\b{}\\b'.format(s_),w,message)\n",
    "    \n",
    "    # If a country has NOTAMs in two languages, the english text starts with the phrase \"english text/english version\"\n",
    "    if \"english text\" in message:\n",
    "        #print(\"TEXT\")\n",
    "        e_t = []\n",
    "        for m_ in message.split(\"english text\"):\n",
    "            for sent_ in sent_tokenize(m_):\n",
    "                if detect(sent_) == \"en\":\n",
    "                    e_t.append(sent_)\n",
    "        message = \"\".join(e_t)\n",
    "\n",
    "    elif \"english version\" in message:\n",
    "        #print(\"VERSION\")\n",
    "        e_t = []\n",
    "        for m_ in message.split(\"english version\"):\n",
    "            for sent_ in sent_tokenize(m_):\n",
    "                if detect(sent_) == \"en\":\n",
    "                    e_t.append(sent_)\n",
    "        \n",
    "        message = \"\".join(e_t)\n",
    "        #message = message.split(\"english version\")[1]\n",
    "    elif detect(message) != \"en\":\n",
    "        message = \"\"\n",
    "\n",
    "    #Start of string other than character or digit\n",
    "    #message = re.sub(r'[^ 0-9a-z]', ' ', message)\n",
    "    #message = message.translate(message.maketrans('', '', string.punctuation)) #extra punctuations removal\n",
    "    message = re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", str(message))\n",
    "    \n",
    "    # Replace numbered points with comma\n",
    "    message = re.sub(\"(\\s\\d+\\.)\",\", \", str(message))\n",
    "    # Remove unnecessary white space\n",
    "    message = \" \".join(message.split())\n",
    "    \n",
    "    return message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Identify commonly used nouns, verbs, noun chunks and special adverbs such as \"other than\",etc.**\n",
    "\n",
    "- This task was done by manually reading through the messages to identify the commonly used phrases and words such as \"all borders\", \"other than\"\n",
    "- The commonly used nouns and verbs were determined using tags and counter\n",
    "- Once identified we group the verbs and nouns to two categories: positive and negative\n",
    "- To identify complete closure of airports we identify the commonly used nouns and noun chunks\n",
    "- In some cases the message talks about some exception for restrictions using some phrases such as \"this does not apply\", etc. In such cases we don't label the restriction as \"open\"/\"close\" rather leave it as \"na\". \n",
    "\n",
    "NOTE:\n",
    " - So in cases where the clear intent of restriction (open/close) is not known we label the restriction as \"na\" denoting \"not known\". Only the country names mentioned in the NOTAMs are extracted\n",
    " \n",
    " - Only for messages where one of the following nouns/verbs are used are only covered. So not all messages' information are extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_verbs_to_filter = [\"permit\",\"allow\", \"accept\",\"open\",\"lift\",\"exempt\"]\n",
    "neg_verbs_to_filter = [\"ban\", \"exclude\", \n",
    "                   \"refuse\",\"expel\", \"deny\", \"prohibit\", \"cancel\",\"suspend\",\"close\",\"forbid\"]\n",
    "restrict_verb = [\"restrict\"]\n",
    "\n",
    "neg_nouns = [\"suspension\",\"cancellation\",\"ban\"]\n",
    "pos_nouns = [\"opening\",\"enter\",\"resumption\",\"exemption\",\"exempt\",\"alleviation\"]\n",
    "\n",
    "# If the country border is closed to all countries the following nouns and noun_chunks are commonly used\n",
    "nouns_to_filter = [\"passenger\", \"foreigner\", \"traveller\", \"flight\",\"airport\",\"arrival\",\"departure\"]\n",
    "noun_chunks_ = [\"all borders\",\"all flights\",\"civil flight\",\"civil flights\",\"all airports\",\"the border\",\"general aviation\",\"ga\",\"commercial flights\",\"all international\"]\n",
    "\n",
    "na_extension = [\"shall hold a negative covid-19 test\",\"unless belong to one of the following\",\"this does not apply\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create Rules**\n",
    "\n",
    "\n",
    "1. use phrase matcher!\n",
    "\n",
    "        <nouns> from <GPE> are <verbs> to land <GPE>\n",
    " \n",
    "2. Look for verbs of interest:\n",
    "\n",
    "        1. Filter for sentences\n",
    "        2. Check if verbs match\n",
    "        3. Get GPEs\n",
    "        4. See if words such as other than, except are present!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Identify complete closure to international flights**\n",
    "\n",
    "* Check if the nouns or noun_chunks mentioned earlier is present in the message.\n",
    "\n",
    "**Identification of airport close/open to specific countries**\n",
    "\n",
    "* Check if positive/negative verbs are present \n",
    "* Check if positive/negative nouns are present \n",
    "* Check if negation is associated with the verb/noun\n",
    "* Identify exceptions in the sentences \n",
    "* Extract the country mention in the message\n",
    "* Associate the identified tag (open/close/na) with the countries mentioned in the message\n",
    "\n",
    "\n",
    "NOTE:\n",
    "\n",
    "* As mentioned earlier, not all messages' restrictions are labelled. In cases where no foreign country name is mentioned, the exact level of restriction is not extracted unless one of the commonly used nouns are present in the message!\n",
    "\n",
    "* In cases where the exact restriction is not known or if the restriction has some conditions such as travellers should undergo covid test or should be citizens, etc then we classify those restrictions as \"na\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following are some of the examples that are identified using the rules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex1 = nlp(\"- khartoum airport will accept flights as its frequency from egypt, uae, and turkey .\")\n",
    "spacy.displacy.render(ex1, style=\"ent\", jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In the above sentence, the airport is identified as \"open\" to Egypt, UAE and Turkey**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex1 = nlp(\"passenger restrictions: all flight arriving/departing to/from italy must comply with the requirements of the president of the ministerial council of 11 jun, flight to/from states and territories other than - member states of the european union - member states of the schengen agreement - united kingdom of great britain and northern ireland - andorra, principaly of monaco - republic of san marino and vatican city state are still prohibited till 30/06/2020 for those flights prohibited exceptions are reported in art 6 of the same decree\")\n",
    "spacy.displacy.render(ex1, style=\"ent\", jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{'GBR': 'open', 'AND': 'open', 'MCO': 'open', 'SMR': 'open', 'VAT': 'open', 'NEUM': 'close', 'NSCH': 'close'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In the above sentence,the rules identify Italy is closed to Non-EU and Non-Schengen and open to Andorra, Monaco, San Marino and Vatican city**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex1 = nlp(\"all international flights from or to morocco are suspended until further notice except over flight and cargo.\")\n",
    "spacy.displacy.render(ex1, style=\"ent\", jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In the above sentence, since the noun chunk \"all international\" is associated with the verb \"suspended\"- the rules identify Morocco is closed to all countries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "d1 = nlp(\"2) third-country nationals arriving from out of schengen area are not allowed to enter austria.\")\n",
    "img_ = visualise_spacy_tree.create_png(d1)\n",
    "display(Image(img_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def all_country_closure(c,other_countries_in_this_sentence):\n",
    "    '''\n",
    "    look for the nouns and identify if the word \"all\" is associated with it. \n",
    "    Then label the restriction as \"all\" countries\n",
    "    '''\n",
    "    countries = []\n",
    "    if (c.lemma_ == \"to\") | (c.lemma_ == \"for\")| (c.lemma_ == \"from\"):\n",
    "        for c_ in c.children:\n",
    "            if c_.lemma_ in nouns_to_filter:\n",
    "                for c__ in c_.children:\n",
    "                    if (c__.lemma_ == \"all\")& (len(other_countries_in_this_sentence) == 0):\n",
    "                        countries = \"all\"\n",
    "    if c.lemma_ in nouns_to_filter:\n",
    "        for c_ in c.children:\n",
    "            if (c_.lemma_ == \"all\")& (len(other_countries_in_this_sentence) == 0):\n",
    "                countries = \"all\"\n",
    "    #print(countries)\n",
    "    if len(countries) != 0:\n",
    "        return countries\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def country_restrictions(asp_active_df):\n",
    "    each_country = dict()\n",
    "    for cc in asp_active_df.countryCode.unique():#['ITA']:#\n",
    "        master_dict= dict()\n",
    "        each_country[cc] = []\n",
    "        relevant_messages = []\n",
    "        print(cc)\n",
    "        for idx, r in asp_active_df[asp_active_df.countryCode==cc].iterrows():\n",
    "\n",
    "            current_country = r['countryCode']\n",
    "\n",
    "            doc = nlp(r['cleaned_message_with_punct']) \n",
    "\n",
    "            # Get all country names mentioned in the message\n",
    "            other_countries = [t.ent_id_ for t in doc if (t.ent_type_ == \"GPE\") & (t.ent_id_ != '') & (t.ent_id_ != current_country)]\n",
    "\n",
    "            #nltk sent tokenise is better\n",
    "            for sent_ in sent_tokenize(doc.text):\n",
    "                doc_ = nlp(sent_)\n",
    "            #for doc_ in doc.sents:\n",
    "                tag = []\n",
    "                tag_single = \"\"\n",
    "                countries = []\n",
    "                countries_dt = {}\n",
    "                specific_country = []\n",
    "                for t in doc_:\n",
    "                    other_countries_in_this_sentence = [t.ent_id_ for t in doc_ if (t.ent_type_ == \"GPE\") & (t.ent_id_ != '') & (t.ent_id_ != current_country)]\n",
    "                    # If one of the positive verbs is present then we tag it as open unless a negation is present in the children\n",
    "                    # Sometimes open is used as adjective int he sentence\n",
    "                    if ((t.pos_ == \"VERB\") & ((t.lemma_ in pos_verbs_to_filter))) | ((t.pos_ == \"ADJ\") & (t.lemma_ == \"open\")):\n",
    "                        tag_ = \"open\"\n",
    "                        # if negation is present in one of the children associated with the verb we invert the tag\n",
    "                        for c_ in t.children:\n",
    "                            if c_.dep_ == \"neg\":\n",
    "                                tag_ = \"close\"\n",
    "                            #print(c_)\n",
    "                            countries_ = all_country_closure(c_,other_countries_in_this_sentence)\n",
    "                            if countries_ is not None:\n",
    "                                countries.append(countries_)\n",
    "                                countries_dt[countries_] = tag_\n",
    "                        tag.append(tag_)\n",
    "                        tag_single = tag_\n",
    "                        for potential_country in t.subtree:\n",
    "                            if (potential_country.ent_type_==\"GPE\")&(potential_country.ent_id_ != '')& (potential_country.ent_id_ != current_country)& ('following' not in [tc.text for tc in potential_country.lefts]):\n",
    "                                specific_country.append(potential_country.ent_id_)\n",
    "\n",
    "                    # If one of the negative verbs is present then we tag it as close unless a negation is present in the children\n",
    "                    elif ((t.pos_ == \"VERB\") & (t.lemma_ in neg_verbs_to_filter)):\n",
    "                        tag_ = \"close\"\n",
    "                        # if negation is present in one of the children associated with the verb we invert the tag\n",
    "                        for c_ in t.children:\n",
    "                            if c_.dep == \"neg\":\n",
    "                                tag_ = \"open\"\n",
    "\n",
    "                            countries_ = all_country_closure(c_,other_countries_in_this_sentence)\n",
    "                            #print(countries_)\n",
    "                            if countries_ is not None:\n",
    "                                countries.append(countries_)\n",
    "                                countries_dt[countries_] = tag_\n",
    "                        tag.append(tag_)\n",
    "                        tag_single = tag_\n",
    "                        for potential_country in t.subtree:\n",
    "                            if (potential_country.ent_type_==\"GPE\")&(potential_country.ent_id_ != '')& (potential_country.ent_id_ != current_country)& ('following' not in [tc.text for tc in potential_country.lefts]):\n",
    "                                specific_country.append(potential_country.ent_id_)\n",
    "\n",
    "                    # If one of the positive nouns is present then we tag it as open unless a negation is present in the children\n",
    "                    if ((t.pos_ == \"NOUN\") & (t.lemma_ in pos_nouns)):\n",
    "                        tag_ = \"open\"\n",
    "                        # if negation is present in one of the children associated with the noun we invert the tag\n",
    "                        for c_ in t.children:\n",
    "                            if c_.dep_ == \"neg\":\n",
    "                                tag_ = \"close\"\n",
    "                            countries_ = all_country_closure(c_,other_countries_in_this_sentence)\n",
    "                            if countries_ is not None:\n",
    "                                countries.append(countries_)\n",
    "                                countries_dt[countries_] = tag_\n",
    "                        tag.append(tag_)\n",
    "                        tag_single = tag_\n",
    "                        for potential_country in t.subtree:\n",
    "                            if (potential_country.ent_type_==\"GPE\")&(potential_country.ent_id_ != '')& (potential_country.ent_id_ != current_country)& ('following' not in [tc.text for tc in potential_country.lefts]):\n",
    "                                specific_country.append(potential_country.ent_id_)\n",
    "\n",
    "                    # If one of the negative nouns is present then we tag it as close unless a negation is present in the children\n",
    "                    elif ((t.pos_ == \"NOUN\") & (t.lemma_ in neg_nouns)):\n",
    "                        tag_ = \"close\"\n",
    "                        # if negation is present in one of the children associated with the verb we invert the tag\n",
    "                        for c_ in t.children:\n",
    "                            if c_.dep == \"neg\":\n",
    "                                tag_ = \"open\"\n",
    "                            countries_ = all_country_closure(c_,other_countries_in_this_sentence)\n",
    "                            if countries_ is not None:\n",
    "                                countries.append(countries_)\n",
    "                                countries_dt[countries_] = tag_\n",
    "                        tag.append(tag_)\n",
    "                        tag_single = tag_\n",
    "                        for potential_country in t.subtree:\n",
    "                            if (potential_country.ent_type_==\"GPE\")&(potential_country.ent_id_ != '')& (potential_country.ent_id_ != current_country)& ('following' not in [tc.text for tc in potential_country.lefts]):\n",
    "                                specific_country.append(potential_country.ent_id_)\n",
    "\n",
    "                    #if ((t.pos_ == \"VERB\") & (t.lemma_ in restrict_verb)):\n",
    "                    #    tag_ = 'restrict'\n",
    "                    #    tag.append('restrict')\n",
    "                    #    tag_single = tag_\n",
    "\n",
    "                    # In cases where the complete restriction is mentioned using one of the noun chunks we identify those mentions and then label the restriction to \"all\" countries\n",
    "                    for current_nc in doc_.noun_chunks: \n",
    "                        for nc_ in noun_chunks_:\n",
    "                            if nc_ == current_nc.text:\n",
    "                                for vs in current_nc.root.ancestors:\n",
    "                                    if ((vs.pos_ == \"VERB\")& ((t.lemma_ in pos_verbs_to_filter)| (t.lemma_ in pos_verbs_to_filter)))& (len(other_countries_in_this_sentence) == 0):\n",
    "                                        countries.append(\"all\")\n",
    "                                        countries_dt[\"all\"] = tag_single\n",
    "                                    if ((vs.pos_ == \"NOUN\")& ((t.lemma_ in pos_nouns)| (t.lemma_ in neg_nouns)))& (len(other_countries_in_this_sentence) == 0):\n",
    "                                        countries.append(\"all\")\n",
    "                                        countries_dt[\"all\"] = tag_single\n",
    "\n",
    "                # Potential country tags\n",
    "                if ((tag_single == \"open\")|(tag_single == \"close\")) & (len(specific_country)>0):\n",
    "                    for sc in specific_country:\n",
    "                        countries_dt[sc] = tag_single\n",
    "\n",
    "                # In some messages the word \"non\" is used to denote negation for countries. These mentions are identified and the tag is inverted\n",
    "                for ix,sus_c in enumerate(doc_):\n",
    "                    if (sus_c.ent_type_==\"GPE\")&(sus_c.ent_id_ != '')& (sus_c.ent_id_ != current_country) &(sus_c.ent_id_ not in countries_dt.keys())& ('following' not in [tc.text for tc in sus_c.lefts]):\n",
    "                        country_prefix = ''\n",
    "                        for ix_ in range(ix-5,ix):\n",
    "                            # Negation for NON mentions\n",
    "                            if (doc_[ix_].lemma_ == \"non\"):\n",
    "                                if sus_c.ent_id_ in ['EUM','SCH']:\n",
    "                                    country_prefix = \"N\"\n",
    "                                else:\n",
    "                                    country_prefix = \"negate\"\n",
    "                        if country_prefix == \"N\":\n",
    "                            countries.append('N'+sus_c.ent_id_)\n",
    "                            countries_dt['N'+sus_c.ent_id_] = tag_single#f_tag\n",
    "                        elif country_prefix == \"negate\":\n",
    "                            countries.append(sus_c.ent_id_)\n",
    "                            if tag_single == \"open\":\n",
    "                                countries_dt[sus_c.ent_id_] = \"close\"#f_tag\n",
    "                            elif tag_single == \"close\":\n",
    "                                countries_dt[sus_c.ent_id_] = \"open\"\n",
    "                            else:\n",
    "                                countries_dt[sus_c.ent_id_] = \"na\"\n",
    "                        else:\n",
    "                            countries.append(sus_c.ent_id_)\n",
    "                            if tag_single == \"open\":\n",
    "                                countries_dt[sus_c.ent_id_] = \"open\"#f_tag\n",
    "                            elif tag_single == \"close\":\n",
    "                                countries_dt[sus_c.ent_id_] = \"close\"\n",
    "                            else:\n",
    "                                countries_dt[sus_c.ent_id_] = \"na\"\n",
    "\n",
    "                # Negation for exceptions\n",
    "                # Tag is inverted for countries that precede with one of the following words: except, exception, outside, out, other, unless\n",
    "                negation_countries =[]\n",
    "                for idx,s in enumerate(doc_):\n",
    "                    if (s.lemma_ ==\"except\")|(s.lemma_ == \"exception\")|(s.lemma_ == \"outside\")|(s.lemma_ == \"out\")|(s.lemma_ == \"other\")|(s.lemma_ == \"unless\")|(s.lemma_ == \"apart\"):\n",
    "                        for _ in range(idx+1,idx+25):\n",
    "                            #print(_)\n",
    "                            try:\n",
    "                                if (doc_[_].ent_type_==\"GPE\")&(doc_[_].ent_id_ != '')& (doc_[_].ent_id_ != current_country)& ('following' not in [tc.text for tc in doc_[_].lefts]):\n",
    "                                    for child in doc_[_].subtree:\n",
    "                                        if (child.ent_type_==\"GPE\")&(child.ent_id_ != '')& (child.ent_id_ != current_country):\n",
    "                                            # print(child,child.ent_id_,countries_dt[child.ent_id_])\n",
    "                                            negation_countries.append(child.ent_id_)\n",
    "                            except Exception as e:\n",
    "                                pass\n",
    "                                #print(str(e))\n",
    "                if len(negation_countries) > 0:\n",
    "                    #print(\"NEGATION\")\n",
    "                    for neg_country in set(negation_countries):\n",
    "                        if neg_country in countries_dt.keys():\n",
    "                            if (\"EUM\" in neg_country)|(\"SCH\" in neg_country):\n",
    "                                countries_dt[\"N\"+ neg_country] = countries_dt[neg_country]\n",
    "                                del countries_dt[neg_country]\n",
    "                            elif countries_dt[neg_country] == 'open':\n",
    "                                countries_dt[neg_country] = 'close'\n",
    "                            elif countries_dt[neg_country] == 'close':\n",
    "                                countries_dt[neg_country] = 'open'\n",
    "                            else:\n",
    "                                countries_dt[neg_country] == 'na'\n",
    "\n",
    "                #if the exact intent of the restriction is uncles tag it as \"na\"\n",
    "                for s in na_extension:\n",
    "                        if s in doc_.text:\n",
    "                            tag.append(\"na\")\n",
    "\n",
    "                # To overocme contradicting messages we set the flag to na\n",
    "                if (len(set(tag)) >1) &(len(countries_dt.keys()) > 0):\n",
    "                    for key_, value_ in countries_dt.items():\n",
    "                        countries_dt[key_] = \"na\"\n",
    "\n",
    "                # consolidate the restrictions by country\n",
    "                if len(countries_dt.keys()) > 0 :\n",
    "                    each_country[cc].append(countries_dt)\n",
    "    return each_country"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting the results to a dataframe:\n",
    "\n",
    "- As we are extracting the information from each sentence, some countries might have contradicting tags (both open and close associated to the same country. \n",
    "  In such cases we simply label the restriction as \"1\"(restriction) if regions such as EU, Scandinavia or as \"2\" (special) for country specific mention\n",
    "\n",
    "- The restriction levels used are given below:\n",
    "\n",
    "    * 4  - home\n",
    "    * 3  - open \n",
    "    * 2  - special\n",
    "    * 1  - restriction\n",
    "    * 0  - close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def integrate_restriction_to_dataframe(each_country):\n",
    "    master_dict = dict()\n",
    "    for main_c in each_country.keys():\n",
    "        master_dict[main_c] = dict()\n",
    "        for dt in each_country[main_c]:\n",
    "            for k,v in dt.items():\n",
    "                if not k in master_dict[main_c].keys():\n",
    "                    master_dict[main_c][k] = []\n",
    "                master_dict[main_c][k].append(v)\n",
    "\n",
    "\n",
    "    # Remove 'all' if other country is mentioned\n",
    "    for country__ in master_dict.keys():\n",
    "        if (len(master_dict[country__].keys()) > 1) & ('all' in master_dict[country__].keys()):\n",
    "                master_dict[country__].pop(\"all\")\n",
    "    \n",
    "    for country__ in master_dict.keys():\n",
    "        non_eu_countries = {'NEU','NEUM','NSCH'}.intersection(set(master_dict[country__].keys()))\n",
    "        set_all_non_eu_to_na = 0\n",
    "        if len(non_eu_countries) > 0:\n",
    "            for cc in non_eu_countries:\n",
    "                if (set(master_dict[country__][cc]) == {'na'})|(set(master_dict[country__][cc]) == {'restrict'}) | (len(set(master_dict[country__][cc])) >1):\n",
    "                    #print(master_dict[country__][cc],cc)\n",
    "                    #print(\"NEU--\")\n",
    "                    set_all_non_eu_to_na = 1\n",
    "\n",
    "            if set_all_non_eu_to_na == 1:\n",
    "                for cc in non_eu_countries:\n",
    "                    #print(country__,cc)\n",
    "                    #print(master_dict[country__][cc])\n",
    "                    master_dict[country__][cc] = ['na']\n",
    "\n",
    "    for country__ in master_dict.keys():\n",
    "        eu_countries = {'EUM','SCH'}.intersection(set(master_dict[country__].keys()))\n",
    "        set_all_eu_to_na = 0\n",
    "        if len(eu_countries) > 0:\n",
    "            for cc in eu_countries:\n",
    "                if (set(master_dict[country__][cc]) == {'na'})|(set(master_dict[country__][cc]) == {'restrict'}) | (len(set(master_dict[country__][cc])) >1):\n",
    "                    #print(master_dict[country__][cc],cc)\n",
    "                    #print(\"EU--\")\n",
    "                    set_all_eu_to_na = 1\n",
    "\n",
    "        if set_all_eu_to_na == 1:\n",
    "            for cc in eu_countries:\n",
    "                #print(country__)\n",
    "                #print(master_dict[country__][cc],cc)\n",
    "                master_dict[country__][cc] = ['na']\n",
    "\n",
    "    res_dt = dict()\n",
    "\n",
    "    for k in master_dict.keys():\n",
    "        res_dt[k] = dict()\n",
    "        european_c_ = list(set(european_regions).intersection(set(master_dict[k].keys())))\n",
    "        if len(european_c_) > 0:\n",
    "            for e_c_ in european_c_:\n",
    "                countries = european_regions_dt[e_c_]\n",
    "                for ec_ in countries:\n",
    "                    #if :\n",
    "                    #    res_dt[k][ec_] = -1#\"conditions\" #-1#\n",
    "                    if set(master_dict[k][e_c_]) == {'open'}:\n",
    "                        res_dt[k][ec_] = 3#\"open\" #1#\n",
    "                    elif set(master_dict[k][e_c_]) == {'close'}:\n",
    "                        res_dt[k][ec_] = 0#\"close\" #0#\n",
    "                    elif (set(master_dict[k][e_c_]) == {\"na\"})|(set(master_dict[k][e_c_]) == {\"restrict\"})|(len(set(master_dict[k][e_c_])) > 1):\n",
    "                        res_dt[k][ec_] = 1#\"other\" #-2#\n",
    "\n",
    "\n",
    "        if \"all\" in master_dict[k].keys():\n",
    "            for all_c in all_countries['Alpha-3 code'].unique():\n",
    "                #if :\n",
    "                #    res_dt[k][all_c] = -1#\"conditions\" #-1#\n",
    "                if set(master_dict[k][\"all\"]) == {'open'}:\n",
    "                    res_dt[k][all_c] = 3#\"open\" #1#\n",
    "                elif set(master_dict[k][\"all\"]) == {'close'}:\n",
    "                    res_dt[k][all_c] = 0#\"close\" #0#\n",
    "                elif (set(master_dict[k][\"all\"]) == {\"na\"})|(set(master_dict[k][\"all\"]) == {\"restrict\"})|(len(set(master_dict[k][\"all\"])) > 1):\n",
    "                    res_dt[k][all_c] = 1#\"other\" #-2#\n",
    "\n",
    "                #restrictions_dict[cc][all_c]['message'].append(relevant_messages)\n",
    "\n",
    "        for sub_o in master_dict[k].keys():\n",
    "            if (sub_o not in european_regions) &(sub_o != 'all'):\n",
    "                #if len(set(master_dict[k][sub_o])) > 1:\n",
    "                #    res_dt[k][sub_o] = 2#\"special\" #2#\n",
    "                if set(master_dict[k][sub_o]) == {'open'}:\n",
    "                    res_dt[k][sub_o] = 3#\"open\" #1#\n",
    "                elif set(master_dict[k][sub_o]) == {'close'}:\n",
    "                    res_dt[k][sub_o] = 0#\"close\" #0#\n",
    "                elif (set(master_dict[k][sub_o]) == {\"na\"})|(set(master_dict[k][sub_o]) == {\"restrict\"}) |(len(set(master_dict[k][sub_o])) > 1) :\n",
    "                    res_dt[k][sub_o] = 2\n",
    "                #elif (set(master_dict[k][sub_o]) == {\"na\"})|(set(master_dict[k][sub_o]) == {\"restrict\"}):\n",
    "                #    res_dt[k][sub_o] = -2#\"other\" #-2#\n",
    "                #restrictions_dict[cc][k]['message'].append(relevant_messages)\n",
    "        res_dt[k][k] = 4#'home' #3#\n",
    "        res_dt[k]['XYZ'] = 0\n",
    "\n",
    "    res_df = pd.DataFrame(res_dt)\n",
    "\n",
    "    closure_df = pd.DataFrame()\n",
    "    for col_c in res_df.columns:\n",
    "\n",
    "        df_ = pd.DataFrame(res_df[col_c])\n",
    "        df_['home'] = col_c\n",
    "        df_.reset_index(inplace=True)\n",
    "        df_.rename(columns={'index':'other',col_c:'restriction'},inplace=True)\n",
    "        closure_df = pd.concat([closure_df,df_])\n",
    "    \n",
    "\n",
    "    return closure_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DATA_FOLDER = \"/project_data/data_asset/notams/\"\n",
    "\n",
    "closure_timeline_df = pd.DataFrame()\n",
    "\n",
    "for file in os.listdir(DATA_FOLDER):\n",
    "    asp_active_df = pd.read_csv(os.path.join(DATA_FOLDER,file))\n",
    "    \n",
    "    asp_active_df['cleaned_message_with_punct'] = None\n",
    "\n",
    "    for idx,row in asp_active_df.iterrows():\n",
    "        if row['message'] is not None:\n",
    "            message = row['message'].lower().strip()\n",
    "\n",
    "            message_ = clean_message_keep_punct(message)\n",
    "\n",
    "            asp_active_df.at[idx,\"cleaned_message_with_punct\"] = message_\n",
    "    \n",
    "    # Each file's restrictions\n",
    "    each_country_dict = country_restrictions(asp_active_df)\n",
    "    \n",
    "    # Integrate the restrictions to a dataframe\n",
    "    closure_df = integrate_restriction_to_dataframe(each_country_dict)\n",
    "    \n",
    "    # Include the restrictions the other way round too\n",
    "    closure_df['external_countries'] = closure_df['home']\n",
    "    closure_df['external_restriction'] = closure_df['restriction']\n",
    "    for a,r in closure_df.groupby('other'):\n",
    "        if not a in list(r['external_countries']):\n",
    "            new_row = pd.DataFrame({'other':a,'external_countries':a,'external_restriction':4},index=[0])\n",
    "            closure_df = closure_df.append(new_row,ignore_index=True)\n",
    "    \n",
    "    #Add time information\n",
    "    closure_df['download_date'] = asp_active_df['download_date'].unique()[0]\n",
    "    closure_df['active_week'] = asp_active_df['active_week'].unique()[0]\n",
    "    \n",
    "    print(len(closure_df),file)\n",
    "    \n",
    "    \n",
    "    \n",
    "    closure_timeline_df = pd.concat([closure_timeline_df,closure_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "closure_timeline_df.to_csv(\"/project_data/data_asset/country_closure_timeline.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "closure_timeline_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data dictionary**\n",
    "\n",
    "The restrictions results are generated on a weekly basis.\n",
    "\n",
    "|column name| description|\n",
    "|-----|----|\n",
    "|download_date| download date of NOTAMs data|\n",
    "| active_week| active week number of NOTAMs data|\n",
    "|home| NOTAMs restrictions for this country|\n",
    "|other| foreign country mentioned in the selected country(home)|\n",
    "|restriction|  type of restriction: 0-closed;1-country mentioned as EU/non-EU,etc but restriction level not clear; 2- explicit mention of country but restriction level not clear; 3- open; 4- home country|\n",
    "|external_countries| this column is the same as \"home\" column and has rows where some countries in \"other\" are not available in \"home\"|\n",
    "|external_restriction| this column is the same as \"restriction\" column and has rows where some restrictions in \"other\" are not available in \"restriction\"|\n",
    "\n",
    "\n",
    "**Columns to be used for visualisation on dashboard**\n",
    "\n",
    "1. Restrictions imposed by a country:\n",
    "    \n",
    "    To visualise the restrictions imposed by a country we use the columns: \"home\", \"other\" and \"restriction\". The user is allows to select a country of choice from the list of countries in \"home\" column and the world map displays the \"other\" column countries having a colour scale corresponding to \"restriction\" column\n",
    "    \n",
    "    \n",
    "2. Restrictions imposed on a country by foreign countries:\n",
    "\n",
    "    To visualise the restrictions imposed on a country the columns \"home\" and \"other\" have to be interchanged. As there are more country names in \"other\" column than in \"home\" column, two additional columns \"external_countries\" and \"external_restriction\" were added to add the missing rows in \"home\" and \"restriction\" columns.\n",
    "    So, let the user select a country from the list of countries in \"other\" column and display the \"external_countries\" column countries having a colour scale corresponding to \"external_restriction\" column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Author**\n",
    "\n",
    "\n",
    "* Shri Nishanth Rajendran - AI Development Specialist, R² Data Labs, Rolls Royce\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
